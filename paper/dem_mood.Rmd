---
output: 
  bookdown::pdf_document2:
    fig_caption: yes
    keep_tex: no
    toc: no
    number_sections: no
    latex_engine: xelatex
    # pandoc_args: --lua-filter=multiple-bibliographies.lua
title: |
  | Revisiting the Evidence of 
  | Thermostatic Response to Democratic Change:
  | Degrees of Democratic Support or 
  | Degrees of Researcher Freedom?
thanks: "Corresponding author: [yuehong-tai@uiowa.edu](mailto:yuehong-tai@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`.  Replication materials and complete revision history may be found at [https://github.com/fsolt/dem_mood](https://github.com/fsolt/dem_mood)."
author:
- Yue Hu
  # affiliation: Tsinghua University
- Yuehong Cassandra Tai
  # affiliation: University of Iowa
- Frederick Solt
  # affiliation: University of Iowa
anonymous: true  
abstract: "'Janitor work'---getting data into a format appropriate for analysis---has grown increasingly important as political science research has come to depend on data drawn from hundreds of sources.  One tempting solution is to simply enter data by hand, but this approach raises serious risks of data-entry error, a difficult-to-catch problem with the potential to fatally undermine our conclusions. Underscoring these points, we identify data-entry errors in a prominent recent article, Claassen's 2020 study examining how changes in democracy influence public support for democracy, and show that when these errors are corrected, its models provide no support for its conclusions.  Researchers should refrain from hand-entering data as much as possible, and we offer additional suggestions for avoiding errors."
keywords: ""
tables: true # enable longtable and booktabs
citation_package: natbib
fontsize: 12pt
indent: true
linestretch: 1.5 # double spacing using linestretch 1.5
bibliography: [dem_mood_text.bib, p_dcpo_dem_data.bib]
# \dummy{`r file.path(getwd(), list.files(getwd(), ".bib$", recursive = TRUE))`}
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/american-political-science-association.csl
biblio-style: apsr
citecolor: black
linkcolor: black
endnote: no
header-includes:
      - \usepackage{array}
      - \usepackage{caption}
      - \usepackage{graphicx}
      - \usepackage{siunitx}
      - \usepackage{colortbl}
      - \usepackage{multirow}
      - \usepackage{hhline}
      - \usepackage{calc}
      - \usepackage{tabularx}
      - \usepackage{threeparttable}
      - \usepackage{wrapfig}
      - \renewcommand{\topfraction}{.85}
      - \renewcommand{\bottomfraction}{.7}
      - \renewcommand{\textfraction}{.15}
      - \renewcommand{\floatpagefraction}{.66}
      - \usepackage{pdflscape} #\usepackage{lscape} better for printing, page displayed vertically, content in landscape mode, \usepackage{pdflscape} better for screen, page displayed horizontally, content in landscape mode
      - \newcommand{\blandscape}{\begin{landscape}}
      - \newcommand{\elandscape}{\end{landscape}}
      - \setcounter{topnumber}{3}
      - \setcounter{bottomnumber}{3}
      - \setcounter{totalnumber}{4}
---
	
\captionsetup[figure]{list=no}
```{r setup, include=FALSE}
options(tinytex.verbose = TRUE)
chooseCRANmirror(graphics = FALSE, ind = 1)

knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE,
    cache = TRUE,
    dpi = 300
)

if (!require(cmdstanr)) {
  install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/",
                                         getOption("repos")))
  library(cmdstanr)
  install_cmdstan() # C++ toolchain required; see https://mc-stan.org/cmdstanr/articles/cmdstanr.html
}

if (!require(pacman))
    install.packages("pacman")
library(pacman)
  
p_install(janitor, force = FALSE)
p_install_gh(c("fsolt/DCPOtools"), force = FALSE)

# load all the packages you will use below
p_load(
    # analysis
    cmdstanr,
    plm,
    osfr,
    
    # presentation
    gridExtra,
    modelsummary,
    dotwhisker,
    latex2exp,
    ggh4x,
    
    # data wrangling
    DCPOtools,
    janitor,
    countrycode,
    here,
    broom,
    tidyverse,
    furrr,
    glue)

# Functions preload
set.seed(313)

## data set-up functions
# dichotomize data according to coding rule and missing-data treatment
dichotomize <- function(df, 
                        by = c("highest", "median", "orig", "lowest"),
                        miss = c("mar", "lower", "upper", "theory")) {
    if (miss == "mar") {
        df <- df %>% 
            filter(!r == -1) # missing at random so omitted
    } else if (miss == "upper") { # all missing recoded as supporting democracy 
        df <- df %>% 
            mutate(r = case_when(r == -1 ~ 999,
                                 TRUE ~ r))
    } else if (miss == "theory") {
        df <- df %>% 
            mutate(r = case_when(r == -1 & dem == 0 ~ 999, # code non-respondents in 
                                 TRUE ~ r))  # autocracies as supporting democracy
    } # else miss == "lower" & all missing answers remain coded as unsupportive
    
    if (by == "highest") {
        df1 <- df %>%
            group_by(country, year, item) %>% 
            summarize(x = sum(ifelse(r == highest_r, n, 0)),
                      samp = sum(n),
                      survey = first(survey),
                      .groups = "drop_last") %>%
            ungroup() %>%
            filter(x > 0)
    } else if (by == "median") {
        df1 <- df %>% 
            group_by(country, year, item) %>% 
            summarize(x = sum(ifelse(r > median_r, n, 0)),
                      samp = sum(n),
                      survey = first(survey),
                      .groups = "drop_last") %>%
            ungroup() %>%
            filter(x > 0)
    } else if (by == "orig") {
        df1 <- df %>% 
            group_by(country, year, item) %>% 
            summarize(x = sum(ifelse(r > orig_r, n, 0)),
                      samp = sum(n),
                      survey = first(survey),
                      .groups = "drop_last") %>%
            ungroup() %>%
            filter(x > 0)
    } else if (by == "lowest") {
        df1 <- df %>% 
            group_by(country, year, item) %>% 
            summarize(x = sum(ifelse(r > lowest_r, n, 0)),
                      samp = sum(n),
                      survey = first(survey),
                      .groups = "drop_last") %>%
            ungroup() %>%
            filter(x > 0)
    }
    
    df2 <- df1 %>% 
        mutate(by = by,
               miss = miss)
    return(df2)
}

format_claassen <- function(claassen_data) {
    claassen_stan <- map(claassen_data, function(dat) {
        one_stan_input <- list(  N       = nrow(dat),
                                 K       = dplyr::n_distinct(dat$item),
                                 T       = max(dat$year) - min(dat$year) + 1,
                                 J       = dplyr::n_distinct(dat$country),
                                 P       = max(as.numeric(as.factor(paste(dat$country, dat$item)))),
                                 jj      = as.numeric(as.factor(dat$country)),
                                 kk      = as.numeric(as.factor(dat$item)),
                                 tt      = dat$year - min(dat$year) + 1,
                                 pp      = as.numeric(as.factor(paste(dat$country, dat$item))),
                                 x       = round(dat$x),
                                 samp    = round(dat$samp),
                                 data    = dat)
    })
    return(claassen_stan)
}

## analyzing and presenting functions
# Variance Calculation ------------------------------------------------------------------

## Beck-Katz panel-corrected standard errors
vcovHC_se <-  function(x) {
    plm::vcovHC(x, method="arellano", cluster="group") %>%  #default setting
        diag() %>% 
        sqrt()
}

# Tabulation -----------------------------------------------------------------------
na_types_dict <- list("r" = NA_real_,
                      "i" = rlang::na_int,
                      "c" = NA_character_,
                      "l" = rlang::na_lgl)

# A function that converts a string to a vector of NA types.
# e.g. "rri" -> c(NA_real_, NA_real_, rlang::na_int)
parse_na_types <- function(s) {
    
    positions <- purrr::map(
        stringr::str_split(s, pattern = ""), 
        match,
        table = names(na_types_dict)
    ) %>%
        unlist()
    
    na_types_dict[positions] %>%
        unlist() %>%
        unname()
}

# A function that, given named arguments, will make a one-row
# tibble, switching out NULLs for the appropriate NA type.
as_glance_tibble <- function(..., na_types) {
    
    cols <- list(...)
    
    if (length(cols) != stringr::str_length(na_types)) {
        stop(
            "The number of columns provided does not match the number of ",
            "column types provided."
        )
    }
    
    na_types_long <- parse_na_types(na_types)
    
    entries <- purrr::map2(cols, 
                           na_types_long, 
                           function(.x, .y) {if (length(.x) == 0) .y else .x})
    
    tibble::as_tibble_row(entries)
    
}

tidy.pgmm <- function(x,
                      conf.int = FALSE,
                      conf.level = 0.95,
                      ...) {
    result <- summary(x)$coefficients %>%
        tibble::as_tibble(rownames = "term") %>%
        dplyr::rename(
            estimate = Estimate,
            std.error = `Std. Error`,
            statistic = `z-value`,
            p.value = `Pr(>|z|)`
        )
    
    if (conf.int) {
        ci <- confint(x, level = conf.level) %>% 
            as.data.frame() %>% 
            rownames_to_column(var = "term") %>% 
            dplyr::rename(
                conf.low = `2.5 %`,
                conf.high = `97.5 %`
            )
        result <- dplyr::left_join(result, ci, by = "term")
    }
    
    result
}

glance.plm <- function(x, ...) {
    s <- summary(x)
    as_glance_tibble(
        nobs = stats::nobs(x),
        n.country = pdim(x)$nT$n,
        na_types = "ii"
    )
}

glance.pgmm <- function(x, ...) {
    s <- summary(x)
    as_glance_tibble(nobs = stats::nobs(x),
        n.country = pdim(x)$nT$n,
        n.inst = dim(x$W[[1]])[2],
        na_types = "iii"
    )
}

theme_set(theme_bw())

## dotwhisker::small_multiple() hacks
body(small_multiple)[[19]] <- substitute(
    p <-
        ggplot(
            df,
            aes(
                y = estimate,
                ymin = conf.low,
                ymax = conf.high,
                x = as.factor(model),
                colour = submodel
            )
        ) +
        do.call(geom_pointrange, point_args) +
        ylab("") + xlab("") +
        facet_grid(
            term ~ .,
            scales = "free",
            labeller = label_parsed,
            # enable LaTeX facet labels
            switch = "y"
        ) +             # put facet labels on left
        scale_y_continuous(position = "right") # put axis label on right
)
```

```{r claassen_input, include=FALSE}
if (!file.exists(here::here("data", "supdem raw survey marginals.csv"))) {
  tempfile <- dataverse::get_file_by_doi("doi:10.7910/DVN/HWLW0J/RA8IJC") # AJPS replication file, not included in APSR replication

  writeBin(tempfile, here::here("data", "supdem raw survey marginals.csv"))
  rm(tempfile)
}

# publication data
sd <- read_csv(here::here("data", "supdem raw survey marginals.csv"), col_types = "cdcddcdc") %>% 
  mutate(item_fam = str_extract(tolower(Item), "^[a-z]+"),
         country = countrycode::countrycode(Country, "country.name", "country.name"),
         dataset = "supdem") %>% 
  rename(year = Year, project = Project) %>% 
  with_min_yrs(2) # Selecting data w. at least two years

# corrected data
if (!file.exists(here::here("data", "claassen_input_raw.csv"))) {
  claassen_input_raw <- DCPOtools:::claassen_setup(
    vars = read_csv(here::here("data-raw", "mood_dem.csv"),
                    col_types = "cccccc")) %>% 
    pluck("lower")
  
  write_csv(claassen_input_raw, 
            file = here::here("data",
                              "claassen_input_raw.csv"))
}

claassen_input_raw0 <- read_csv(here::here("data",
                                          "claassen_input_raw.csv"),
                               col_types = "cdcddcd")

claassen_input_raw1 <- claassen_input_raw0 %>%
  filter(!((
    str_detect(item, "army_wvs") &
      # WVS obs identified as problematic by Claassen
      ((country == "Albania" & year == 1998) |
         (country == "Indonesia" &
            (year == 2001 | year == 2006)) |
         (country == "Iran" & year == 2000) |
         (country == "Pakistan" &
            (year == 1997 | year == 2001)) | # 1996 in Claassen
         (country == "Vietnam" & year == 2001)
      )
  ) |
    (
      str_detect(item, "strong_wvs") &
        ((country == "Egypt" & year == 2012) |
           (country == "Iran" &
              (year == 2000 | year == 2007)) | # 2005 in Claassen
           (country == "India") |
           (country == "Pakistan" &
              (year == 1997 | year == 2001)) | # 1996 in Claassen
           (country == "Kyrgyzstan" &
              (year == 2003 | year == 2011)) |
           (country == "Romania" &
              (year == 1998 | year == 2005 | year == 2012)) |
           (country == "Vietnam" & year == 2001)
        )
    ) |
    (
      country %in% c(
        "Puerto Rico",
        "Northern Ireland",
        "SrpSka Republic",
        "Hong Kong SAR China"
      )
    ))) %>%
  with_min_yrs(2)

claassen_input0 <- DCPOtools::format_claassen(claassen_input_raw1)

cri <- claassen_input0$data %>% 
  mutate(p_dcpo = str_extract(survey, "^[a-z]+"), 
         project = case_when(p_dcpo == "afrob" ~ "afb",
                             p_dcpo == "amb" ~ "lapop",
                             p_dcpo == "arabb" ~ "arb",
                             p_dcpo == "asiab" ~ "asb",
                             p_dcpo == "asianb" ~ "asnb",
                             p_dcpo == "neb" ~ "ndb",
                             p_dcpo == "sasianb" ~ "sab",
                             TRUE ~ p_dcpo),
         item_fam = str_extract(item, "^[a-z]+"),
         item_fam = if_else(item_fam == "election", "elec", item_fam),
         dataset = "cri")

supdem_cri <- full_join(sd, cri, by = c("country", "year", "item_fam", "project"))

# While DCPO assigns actual year of survey, Claassen (2020) often uses nominal year
# of wave, so some corrections are required to match observations (~8% of obs);
# we could write about this as a data-entry error, too, but no space to spare,
# so we'll adopt those years instead to facilitate straight-up comparison
no_problems <- inner_join(sd %>% select(-dataset), 
                          cri %>% select(-dataset),
                          by = c("country", "year", "item_fam", "project")) # 3400 obs

needed <- anti_join(sd %>% select(-dataset),
                    cri %>% select(-dataset))                   # 316 obs

available <- anti_join(cri %>% select(-dataset),
                       sd %>% select(-dataset))                # 1336 obs

year_fixes <- left_join(needed,    # 304 obs
                        available,
                        by = c("country", "project", "item_fam")) %>% 
  mutate(diff = year.x - year.y) %>% 
  group_by(country, project, item_fam, year.x) %>% 
  mutate(closest_to_claassen = min(abs(diff))) %>% 
  ungroup() %>% 
  group_by(country, project, item_fam, year.y) %>% 
  mutate(closest_to_dcpo = min(abs(diff))) %>% 
  ungroup() %>% 
  filter(closest_to_claassen == abs(diff) & closest_to_dcpo == abs(diff) & abs(diff) <= 3) %>% 
  filter(!(country == "Egypt" & year.x == 2014 & survey == "afrob5")) %>%  # double matches (it's really afrob6)
  mutate(year = year.y)

claassen_sample <- bind_rows(no_problems, year_fixes) %>% 
    select(country, year, item)

df_apsr <- read_csv(here::here("data", "dem_mood_apsr.csv"),
                     show_col_types = FALSE)

base_input_raw <- claassen_input_raw0 %>%
    right_join(claassen_sample,
               by = join_by(country, year, item)) %>% 
    left_join(df_apsr %>%
                transmute(country = countrycode(Country,
                                                "country.name",
                                                "country.name"),
                          year = Year,
                          dem = Regime_di),
              by = join_by(country, year)) %>% 
    group_by(country, year, item) %>% 
    mutate(highest_r = max(r),
           lowest_r = 1,
           median_r = median(setdiff(r, c(-1, 999))),
           orig_r = case_when((str_detect(survey, "asiab") &
                                   item == "evdemoc_asiab") ~ 1,
                              (str_detect(survey, "pew") &
                                   item == "impdemoc_pew") ~ 3,
                              TRUE ~ median_r)) %>% 
    ungroup()

by <- c("orig", "highest", "median", "lowest") %>% # coding rules
    set_names()

miss <- c("lower", "upper", "theory", "mar") %>% # missing-data treatments
    set_names()

claassen_input_raw <- map(by,
                 \(b) {
                     map(miss,
                         \(m) dichotomize(base_input_raw, b, m))
                 })

claassen_input <- map(by, \(b) {
  rule <- claassen_input_raw %>% 
        pluck(b) 
  format_claassen(rule)
})

rio::export(claassen_input, 
            file = here("data",
                        "claassen_input.rds"))
```

## Intro
Researcher degrees of freedom, reproducibility crisis


## Coding Rules and Democratic Support

With democracy under threat in countries around the world, how the public reacts to democratic erosion is a crucial question.
According to a classic and still vibrant literature, growing experience with democratic governance helps generate robust public support for democracy [see, e.g., @Lipset1959a; @Welzel2013; @Wuttke2022].
@Claassen2020b argues instead that democratic support behaves thermostatically: that increases in democracy yield an authoritarian backlash in the public, while democratic backsliding prompts the public to rally to democracy's cause.

The evidence it offers in support of this latter argument takes advantage of recent advances in modeling public opinion as a latent variable to measure democratic support.
This approach provides estimates of the paper's dependent variable for over one hundred countries for up to nearly three decades, constituting a much larger evidentiary base than any previous study.
These latent-variable estimates, in turn, were based on thousands of nationally aggregated responses to dozens of different questions from cross-national survey projects [@Claassen2020b, 40].
Two pieces of data were collected for each distinct survey item in each country and year it was asked: the number of respondents to give a democracy-supporting response---defined, for ordinal responses, as those above the median value of the scale [@Claassen2020b, Appendix 1.3]---and the total number of respondents to whom the question was posed.
Each of these 7,538 pieces of source data is recorded in a spreadsheet.^[
The article's replication materials include only the latent variable estimates without the original survey aggregates that serve as their source data [see @Claassen2020c]. 
Fortunately, however, the spreadsheet recording these original source data is included in the replication materials for a companion piece that employed the identical estimates [see @Claassen2020d].
]

We re-collected all of the source data for the publication from the original surveys.
We identified the variables of the survey items used by the article within each survey dataset, and then we used an automated process to collect the needed data from the survey datasets while avoiding data-entry errors [see @Solt2018].
In Figure \@ref(fig:comparison), we compare the percentage of respondents to give a democracy-supporting response in the publication spreadsheet with the percentage we found using our automated process of wrangling these same data.
<!-- When points fall along the plot's dotted line, it indicates that the publication's source data and our own automated workflow reported the same percentages. -->
<!-- Points above this diagonal represent observations for which the publication data overestimated the actual percentage of respondents who offered a democracy-supporting response, while points below this line are observations where the publication data underestimated this percentage. -->


<!-- For % of the country-year-item observations, the difference between these percentages was negligible---less than half a percent---yielding points approximately along the plot's dotted line. -->
But for the remaining observations, the difference was often substantial due to data-entry errors in the publication data.
For example, the Asia Barometer asked respondents in 35 country-years to indicate whether they thought "a democratic political system" would be very good, fairly good, or bad for their country.
According to the study's coding rules [see @Claassen2020b, Appendix 1.3], only answers above the median of the response categories should be considered as democracy supporting, yet in this case the lukewarm intermediate category was coded as supporting democracy as well.^[
Although this may be interpreted as an exercise of researcher judgment as to what constitutes a democracy-supporting response rather than a data-entry error, examination of similar answers to similar questions shows that similarly lukewarm responses at and below the median response category  (e.g., in the Arab Barometer, that democracy was "somewhat appropriate" for the country) were coded as not supportive.
]
<!-- This led to overestimations of the percentage of democracy-supporting responses ranging from  to  percentage points and averaging  points. -->

Similarly, the four waves of the Asian Barometer included the following item: "Here is a similar scale of 1 to 10 measuring the extent to which people think democracy is suitable for our country. If 1 means that democracy is completely unsuitable for [name of country] today and 10 means that it is completely suitable, where would you place our country today?"
In accordance with the coding rules of the study, responses of 6 through 10 are considered democracy supporting, and that is how the first, third, and fourth waves of the survey are coded.
For the second wave, however, 5 was erroneously also included among the democracy-supporting responses.
<!-- This data-entry error resulted in overestimates of as much as  percentage points in 9 country-years.  -->

A third example comes from the Pew Global Attitudes surveys' four-point item asking about the importance of living in a country with regular and fair contested elections:
the question wording is "How important is it to you to live in a country where honest elections are held regularly with a choice of at least two political parties? Is it very important, somewhat important, not too important or not important at all?"
In this case, rather than including respondents who gave both responses above the median---"very important" and "somewhat important"---only those respondents who answered "very important" were entered as supporting democracy.
This error caused substantial underreporting of the extent of democratic support in 91 country-years.


According to the study's coding rules, refusing to answer is equivalent to answering in a fashion not supporting democracy [see @Claassen2020b, Appendix 1.3].

Finally, although not depicted on this plot, data-entry errors were also evident in the variable recording the year in which a survey was conducted: these typically reflected differences between the nominal year of a survey wave and when the survey was actually in the field in a particular country.
<!-- This was an issue for some % of the country-year observations. -->


## Consequences for Inference

```{r cm5_output, eval=FALSE, cache=FALSE, include=FALSE, results=FALSE}
# omit eval=FALSE to re-generate the corrected democracy support series
# not evaluated by default because it takes a long while on most machines
cm5 <- cmdstan_model(here("R", "supdem.stan.mod5.stan"))

warmup <- 1000; sampling <- 200

plan(multisession, workers = 12)
future_options <- furrr_options(seed = 324)

orig_output <- future_map(claassen_input %>% 
                                pluck("orig"),
                              \(ci_df) {
                                cm5$sample(
                                  data = ci_df, 
                                  max_treedepth = 14,
                                  adapt_delta = 0.99,
                                  step_size = 0.005,
                                  seed = 324, 
                                  chains = 3, 
                                  parallel_chains = 3,
                                  iter_warmup = warmup,
                                  iter_sampling = sampling,
                                  refresh = warmup/25
                                )
                              },
                              .options = future_options
)

results_path <- here::here(file.path("data", 
                                     "orig"))
dir.create(results_path, 
           showWarnings = FALSE, 
           recursive = TRUE)
map(miss, \(m) {
  dir.create(file.path(results_path, m),
             showWarnings = FALSE, 
             recursive = TRUE)
  mt <- orig_output %>% 
    pluck(m)
  mt$save_data_file(dir = file.path(results_path, m),
                    random = FALSE)
  mt$save_output_files(dir = file.path(results_path, m),
                       random = FALSE)
})

highest_output <- future_map(claassen_input %>% 
                                pluck("highest"),
                              \(ci_df) {
                                cm5$sample(
                                  data = ci_df, 
                                  max_treedepth = 14,
                                  adapt_delta = 0.99,
                                  step_size = 0.005,
                                  seed = 324, 
                                  chains = 3, 
                                  parallel_chains = 3,
                                  iter_warmup = warmup,
                                  iter_sampling = sampling,
                                  refresh = warmup/25
                                )
                              },
                              .options = future_options
)


results_path <- here::here(file.path("data", 
                                     "highest"))
dir.create(results_path, 
           showWarnings = FALSE, 
           recursive = TRUE)
map(miss, \(m) {
  dir.create(file.path(results_path, m),
             showWarnings = FALSE, 
             recursive = TRUE)
  mt <- highest_output %>% 
    pluck(m)
  mt$save_data_file(dir = file.path(results_path, m),
                    random = FALSE)
  mt$save_output_files(dir = file.path(results_path, m),
                       random = FALSE)
})

median_output <- future_map(claassen_input %>% 
                              pluck("median"),
                            \(ci_df) {
                              cm5$sample(
                                data = ci_df, 
                                max_treedepth = 14,
                                adapt_delta = 0.99,
                                step_size = 0.005,
                                seed = 324, 
                                chains = 3, 
                                parallel_chains = 3,
                                iter_warmup = warmup,
                                iter_sampling = sampling,
                                refresh = warmup/25
                              )
                            },
                            .options = future_options
)


results_path <- here::here(file.path("data", 
                                     "median"))
dir.create(results_path, 
           showWarnings = FALSE, 
           recursive = TRUE)
map(miss, \(m) {
  dir.create(file.path(results_path, m),
             showWarnings = FALSE, 
             recursive = TRUE)
  mt <- median_output %>% 
    pluck(m)
  mt$save_data_file(dir = file.path(results_path, m),
                    random = FALSE)
  mt$save_output_files(dir = file.path(results_path, m),
                       random = FALSE)
})

lowest_output <- future_map(claassen_input %>% 
                              pluck("lowest"),
                            \(ci_df) {
                              cm5$sample(
                                data = ci_df, 
                                max_treedepth = 14,
                                adapt_delta = 0.99,
                                step_size = 0.005,
                                seed = 324, 
                                chains = 3, 
                                parallel_chains = 3,
                                iter_warmup = warmup,
                                iter_sampling = sampling,
                                refresh = warmup/25
                              )
                            },
                            .options = future_options
)


results_path <- here::here(file.path("data", 
                                     "lowest"))
dir.create(results_path, 
           showWarnings = FALSE, 
           recursive = TRUE)
map(miss, \(m) {
  dir.create(file.path(results_path, m),
             showWarnings = FALSE, 
             recursive = TRUE)
  mt <- lowest_output %>% 
    pluck(m)
  mt$save_data_file(dir = file.path(results_path, m),
                    random = FALSE)
  mt$save_output_files(dir = file.path(results_path, m),
                       random = FALSE)
})
```

```{r df_apsr_list, eval=FALSE}
# the if() below needs updated for the 16 versions
if (!exists("results_path")) {
  results_path <- here::here("data", "corrected", latest)
  
  # Define OSF_PAT in .Renviron: https://docs.ropensci.org/osfr/articles/auth
  if (!file.exists(file.path(results_path, paste0("supdem.stan.mod5-", latest, "-1.csv")))) {
    dir.create(results_path, showWarnings = FALSE, recursive = TRUE)
    osf_retrieve_node("vmxkn") %>%
      osf_ls_files() %>% 
      filter(str_detect(name, latest)) %>% 
      osf_download(path = here::here("data", "corrected"))
  }
}

load_results <- function(by) {
  b_results_path <- file.path("data", by)
  map(miss,
      \(m) {
        bm_results_path <- file.path("data", by, m)
        as_cmdstan_fit(here(bm_results_path,
                            list.files(bm_results_path, pattern = "csv$")))
      })
}  

orig_output <- load_results("orig")
highest_output <- load_results("highest")
median_output <- load_results("median")
lowest_output <- load_results("lowest")
    
summarize_cm5_results <- function(cm5_input,
                                  cm5_output) {

  miss <- c(lower = "lower", upper = "upper", mar = "mar", theory = "theory"
  )

  res <- map(miss, \(m) {  
    dat <- cm5_input %>% 
      pluck(m) %>% 
      pluck("data")
    
    kcodes <- dat %>%
      dplyr::transmute(country = country,
                       kk = as.numeric(as.factor(country))) %>% 
      unique()
    
    tcodes <- tibble(year = min(dat$year):max(dat$year),
                   tt = year - min(year) + 1)
  
    ktcodes <- dat %>%
      dplyr::group_by(country) %>%
      dplyr::summarize(first_yr = min(year),
                       last_yr = max(year))
    
    fit <- cm5_output %>% 
      pluck(m)
    
    summary_measures <- c("mean", "median", "sd", "mad", "rhat", "ess_bulk", "ess_tail")
    
    suppressWarnings({
      fit$summary("theta",
                  ~posterior::quantile2(., probs = c(.1, .9)),
                  summary_measures) %>%
        dplyr::mutate(tt = as.numeric(gsub("theta\\[(\\d+),\\d+\\]",
                                           "\\1",
                                           variable)),
                      kk = as.numeric(gsub("theta\\[\\d+,(\\d+)\\]",
                                           "\\1",
                                           variable))) %>%
        dplyr::left_join(kcodes, by = "kk") %>%
        dplyr::left_join(tcodes, by = "tt") %>%
        dplyr::left_join(ktcodes, by = "country") %>%
        dplyr::filter(year >= first_yr) %>% # & year <= last_yr
        dplyr::arrange(kk, tt) %>%
        dplyr::select(country, year,
                      starts_with("me"),
                      sd, mad, starts_with("q"),
                      rhat, starts_with("ess"),
                      variable, kk, tt)
    })
  })
  
  return(res)
}

theta_summary <- map(by, \(b) {
  summarize_cm5_results(claassen_input[[b]],
                        get(paste0(b, "_output")))
})


if (!file.exists(here::here("data", "dem_mood_apsr.csv"))) {
    tempfile <- dataverse::get_file_by_doi(filedoi = "doi:10.7910/DVN/FECIO3/ACMGQG") # APSR replication
    
    writeBin(tempfile, here::here("data", "dem_mood_apsr.csv"))
    rm(tempfile)
}

df_apsr <- read_csv(here::here("data", "dem_mood_apsr.csv"),
                     show_col_types = FALSE) %>% 
  mutate(Country = countrycode(Country,
                               "country.name",
                               "country.name"))

df_apsr_list <- map(by,
                    \(b) {
                      map(miss,
                          \(m) {
                            df_apsr %>%
                              left_join(theta_summary[[b]][[m]], 
                                         by = c("Year" = "year",
                                                "Country" = "country")) %>% 
                              mutate(SupDem_trim = mean) %>% 
                              select(Country,
                                     Year,
                                     SupDem_trim,
                                     Libdem_z,
                                     lnGDP_imp) %>% 
                              filter(., complete.cases(.))
                          })
                    })

df_apsr_list[[1]][[1]] <- df_apsr %>% 
  select(Country, Year, SupDem_trim, Libdem_z, lnGDP_imp) %>% 
  filter(., complete.cases(.))
  
rio::export(df_apsr_list, file = here("data",
                                      "df_apsr_list.rds"))
```

Data-entry errors of this sort can yield erroneous conclusions.
After generating the latent variable of democratic support with the corrections to the errors described above, we replicated each of the models presented in @Claassen2020b exactly using both the original and the new version of the latent variable.
The results using the corrected data reveal some support for the classic argument that democracy generates its own demand through the long-run processes of socialization and learning and none at all for a thermostatic relationship.

```{r regression_results}
df_apsr_list <- rio::import(here("data", "df_apsr_list.rds"))

m1_1_list <- map(by,
                 \(b) {
                   map(miss,
                       \(m) {
                         plm(diff(SupDem_trim, lag = 1) ~ 
                               plm::lag(SupDem_trim, 1:2) +
                               diff(Libdem_z, lag = 1) +
                               plm::lag(Libdem_z, 1) +
                               diff(lnGDP_imp, lag = 1) +
                               plm::lag(lnGDP_imp, 1),
                             model = 'pooling',
                             data = pdata.frame(df_apsr_list[[b]][[m]],
                                                index = c("Country", "Year")))
                       })
                 })

m1_1_tidy <- map(by,
                 \(b) {
                   map(miss,
                       \(m) {
                         tidy_result <- tidy(m1_1_list[[b]][[m]],
                                             conf.int = TRUE) %>% 
                           mutate(std.error = vcovHC_se(m1_1_list[[b]][[m]]),
                                  model = paste(b, m),
                                  coding = b,
                                  treatment = m) %>% 
                           filter(str_detect(term, "Libdem"))
                       }) %>% 
                     bind_rows()
                 }) %>% 
  bind_rows()
```

```{r regressionPlot, fig.cap= "The Effects of Democracy on the Change in Public Support", fig.width=7, fig.height=9}
txt_caption <- strwrap("Notes: Replications of Claassen (2020, 47), Table 1, Model 1.1.  The mixed coding rule employed in Claassen (2020) along with that work's assumption that non-responses indicate a lack of support for democracy yields a larger negative point estimate of the coefficient for change in liberal democracy than most other combinations and a larger point estimate of the coefficient for the lagged level of liberal democracy than all other combinations.  In error-correction models like these, both coefficients must be interpreted together; see Figure 2.", width = 115) %>% 
  paste0(sep="", collapse="\n")

m1_1_tidy %>%
  mutate(term = if_else(term == "diff(Libdem_z, lag = 1)",
                        "Delta~Liberal~Democracy",
                        "Liberal~Democracy[t-1]"),
         model = factor(treatment,
                        levels = miss,
                        labels = c("Unsupportive",
                                   "Supportive",
                                   "Oppositional",
                                   "At Random")), 
         submodel = factor(coding,
                           levels = by,
                           labels = c("Mixed",
                                      "Above Median",
                                      "Only Highest",
                                      "All But Lowest"))) %>%
  small_multiple(model_order = c("Unsupportive",
                                   "Supportive",
                                   "Oppositional",
                                   "At Random")) +
  geom_hline(yintercept = 0, colour = "grey50", linetype = 2) +
  scale_color_viridis_d(name = "Coding Rule",
                        option = "magma",
                        end = .8) +
  theme(strip.text.y.left = element_text(angle = 0),
        strip.background = element_rect(colour="white", fill="white"),
        legend.position = c(-.01, 1),
        legend.justification = c(1, 1), 
        legend.title.align = 0.5,
        legend.text = element_text(size = 8),
        legend.background = element_rect(color="gray90"),
        legend.spacing = unit(-5, "pt"),
        legend.key.width = unit(4.5, "mm"),
        legend.key.height = unit(7, "mm"),
        plot.title.position = "plot",
        plot.caption.position = "plot",
        plot.caption = element_text(size = 10, hjust = 0)) +
  guides(x = guide_axis(title = "Non-Response Treatment")) +
  ggtitle("Predicting the Change in Public Democratic Support") +
  labs(caption = txt_caption)
```

Figure \@ref(fig:regressionPlot) is a "small multiple" plot [see @SoltHu2015] showing the results of replicating Model 1.1, the principal model of @Claassen2020b [, 47], with each of the sixteen combinations of coding rule and treatment of survey non-response.
In the top panel, the dots represent point estimates for the coefficients for change in liberal democracy; in the bottom panel, they depict the coefficients for lagged level of liberal democracy.
In both panels, the whiskers show the associated 95% confidence intervals.
Each coding rule is represented by a different color, while the four non-response treatments are shown in separate clusters from left to right.^[
The full results for Model 1.1 can be found in the online Supplementary Materials.]

Models 1.1 through 1.4, which replicate those presented in Table 1 of @Claassen2020b [, 47], examine the effects of overall liberal democracy using error-correction models and first-difference models.
As @Claassen2020b [, 46] notes, the thermostatic theory predicts that the estimated coefficient of the change in liberal democracy will be negative, while the classic theory suggests that lagged levels of liberal democracy will be positive.
When using the original publication data with their data-entry errors, we replicate the results of the article exactly: the coefficients estimated for the change in liberal democracy are large, negative, and statistically significant across all four models, just as the thermostatic theory predicts.
The positive and statistically significant result for the lagged level of liberal democracy found in Model 1.1---supporting the classic theory---disappears when corruption is taken into account in Model 1.2.

When the data-entry errors are corrected, however, the results for these models suggest a very different set of conclusions.
The standard errors shrink across the board, indicating that the models are better estimated in the corrected data.
The positive and statistically significant result for the lagged level of liberal democracy remains in Model 1.1, and the estimate is only slightly smaller than in the publication data.
When corruption is added in Model 1.2, or when the focus shifts from overall liberal democracy to its minoritarian component in Models 2.1 and 2.2, this estimate actually grows slightly larger when using the corrected data rather than the publication data, although it still fails to reach statistical significance.
On the other hand, the estimates for the change in liberal democracy that provided support for the thermostatic theory are much smaller in all of the models.
Models 2.1 through 2.4, which break liberal democracy into its electoral and minoritarian components, similarly yield smaller coefficients for the change in democracy than those found using the publication data.

```{r simulations}
sim_dem_mood <- function(m1_1_result) {
  mod <- m1_1_result
  n.coef =  length(coef(mod)) # 7
  n.yrs = 230
  n.burn = 200
  pred.data = data.frame(Intercept=1,
                         SupDem_m1=0,
                         SupDem_m2=0, 
                         ChgDem=0,
                         Libdem_m1=c(rep(-.5, n.burn),
                                     rep(.5, n.yrs-n.burn)), 
                         ChglogGDP=0,
                         lnGDP_m1=mean(df_apsr_list %>% 
                                         pluck("median") %>% 
                                         pluck("lower") %>% 
                                         pull(lnGDP_imp), na.rm=TRUE),
                         ChgSup=NA,
                         SupDem=NA)
  
  for(i in 1:(n.yrs-1)) {
    pred.data[i, "ChgDem"] = pred.data[i+1, "Libdem_m1"] - pred.data[i, "Libdem_m1"]
    pred.data[i, "ChgSup"] = sum(pred.data[i, 1:n.coef] * coef(mod))
    pred.data[i, "SupDem"] = pred.data[i, "SupDem_m1"] + pred.data[i, "ChgSup"]
    pred.data[i+1, "SupDem_m1"] = pred.data[i, "SupDem"]
    pred.data[i+1, "SupDem_m2"] = pred.data[i, "SupDem_m1"]
  }
  
  n.sims = 5e4
  mod.sims = MASS::mvrnorm(n=n.sims, mu=coef(mod), Sigma=plm::vcovHC(mod))
  yhat = matrix(NA, ncol=n.yrs, nrow=n.sims)
  sup.hat = matrix(NA, ncol=n.yrs, nrow=n.sims)
  for(i in 1:(n.yrs-1)) {
    yhat[, i] = mod.sims %*% t(pred.data[i, 1:n.coef])
    sup.hat[, i] = pred.data[i, "SupDem_m1"] + 
      yhat[, i] +
      rnorm(n.sims, 0, sigma(mod))
  }
  
  sup_hat <- sup.hat - apply(sup.hat, 2, mean, na.rm=TRUE)[n.burn-1]
  
  sim_data11 <- tibble(fd = apply(sup_hat, 2, mean, na.rm=TRUE),
                       u95 = apply(sup_hat, 2, quantile, 0.975, na.rm=TRUE),
                       l95 = apply(sup_hat, 2, quantile, 0.025, na.rm=TRUE),
                       model = "Model 1.1",
                       dem = "Liberal Democracy") %>% 
    mutate(year = row_number() - n.burn) %>% 
    filter(year > -5 & !is.na(fd))

  return(sim_data11)
}

sim_m1_1_list <- map(by,
                     \(b) {
                       map(miss,
                           \(m) {
                             sim_dem_mood(m1_1_list[[b]][[m]]) %>% 
                               mutate(by = factor(b,
                                                  levels = by,
                                                  labels = c("Mixed",
                                                             "Above Median",
                                                             "Only Highest",
                                                             "All But Lowest")),
                                      miss = factor(m,
                                                    levels = miss,
                                                    labels = c("Unsupportive",
                                                               "Supportive",
                                                               "Oppositional",
                                                               "At Random")))
                           }) %>% 
                         bind_rows()
                     }) %>% 
  bind_rows()
```

```{r simulationPlot, fig.cap = "Simulated Effects of Democracy on Changes in Public Democratic Support", fig.width=7, fig.height=9}
txt_caption <- strwrap("Notes: Simulated effects are estimated using coefficients from the models presented in Figure 1. The solid lines indicate the mean simulated effect; the shaded regions indicate the 95% confidence intervals of these effects.", width = 115) %>% 
  paste0(sep="", collapse="\n")

ggplot(data = sim_m1_1_list) +
  geom_hline(aes(yintercept = 0), colour="gray50", linetype="dashed") +
  geom_line(aes(x = year, y = fd)) +
  geom_ribbon(aes(x = year, ymin = l95, ymax = u95, alpha = .6)) +
  facet_grid(rows = vars(miss),
             cols = vars(by),
             switch = "y") +
  scale_y_continuous(position = "right") +
  theme(strip.background = element_rect(colour="white", fill="white"),
        legend.position = "none",
        plot.caption.position = "plot",
        plot.caption = element_text(size = 10, hjust = 0)) +
  ggtitle("Simulating the Effect of a Change in Democracy on Public Democratic Support") +
  guides(x = guide_axis(title = "Year"),
    x.sec = guide_none(title = "Coding Rule"),
    y = guide_axis(title = element_blank()),
    y.sec = guide_none(title = "Non-Response Treatment")) +
  labs(caption = txt_caption)
```

<!-- v good v -->
Like the coefficients of constitutive terms of multiplicative interactions [see, e.g., @Brambor2006], the coefficients of error-correction models cannot be interpreted separately [see @Williams2012].
Figure \@ref(fig:simulationPlot) is similar to Claassen's [-@Claassen2020b, 48] Figure 5.
It depicts simulated effects, in differences not levels for ease of interpretation, of a one standard deviation increase in democracy on the public's support for democracy using the sixteen sets of regression coefficients presented in Figure \@ref(fig:coefficientPlot) with the four different coding rules appearing the columns and the four different missing data treatments in the rows.

In the upper left pane, the combination of mixed coding rule and treating survey non-responses as indicating a lack of support for democracy matches that employed in @Claassen2020b.
The initial drop and slow recovery in the mean of these simulations of public democratic support was the evidence presented for the "thermostatic response of public opinion" and the claim that "little evidence that democracy generates its own demand" [@Claassen2020b, 48].
But the other panes, with a few exceptions, show smaller dips, quicker recoveries, and continued increases; these findings lead to very different conclusions.
Indeed, most of these alternate analyses---eleven of fifteen---show statistically significant increases in democratic support within three decades, the sort of generational change predicted by the classic theory since @Lipset1959.
None of them show statistically significant declines that would lend credence to the argument that democratic support responds thermostatically.^[
Replications of the article's other models can be found in the online Supplementary Materials.]


## Discussion

That support for democracy is different from the other aspects of public opinion that exhibit thermostatic responses should not be surprising: the theory's mechanism does not apply to democratic support.
As originally proposed, the theory demanded a level of political knowledge that the public is well understood to not hold, and as recently re-elaborated it requires political parties to debate the issue as so provide the public with cues as to what is going on [@Atkinson2021, 5-6].
But political parties that work to roll back democracy rarely if ever explicitly argue for that outcome.
Instead, such parties insist that their actions are necessary to protect democracy from pernicious external influences or are needed to reform democracy in ways that will better serve national interests.
And when these parties manage to erode democracy by restricting the freedoms of speech, press, and association, opposing parties and their counterarguments grow less and less likely to even reach the public.
Without open and vigorous debate on the issue, any thermostatic response in democratic support to democratic erosion breaks down.
<!-- ^good -->




The analysis above reveals that data-entry errors are an especially pernicious threat to the credibility of our results.
The threat is a subtle one that is not easily detected.
To discern it requires close scrutiny of every manual entry; merely examining the data and their distribution will uncover few errors [@Barchard2011, 1837-1838].
Although failure to find support for a research hypothesis may prompt us to undertake a such a close review, an analysis that yields statistical significance is unlikely to trigger what will likely be, as in the above example, a time-consuming and difficult effort [see @Gelman2014, 464].
<!-- These different courses put us in 'the garden of forking paths,' rendering our findings suspect even when we only ever perform a single analysis [@Gelman2014, 464]. -->

This leads us to recommend the following steps to reduce possible data-entry errors.
First, __automate data entry__: we suggest researchers to consider reducing reliance on manual data entry and increase the extent to which data wrangling is performed computationally.
Readers should be aware that our suggestion does *not* imply that computers are always superior to people for this purpose. 
Instead, automated coding should always involve sensitive human design and systematic supervision [see @Breznau2021; @Grimmer2015].
Still, given the convenience of automatic data entry in documentation and reproduction, we encourage researchers to use it as much as possible instead of entering data manually to increase the efficacy and transparency of their data processing operations.^[For a systematic discussion of the function of automated data processes in social science, see @Weidmann2023.]

In making this recommendation, we are aware that being open and transparent in this way takes effort [@EngzellRohrer2021].
But as researchers automate more of their data entry, the chances that they can reuse their code in subsequent projects improve.
In fact, many common janitor-work chores already have been packaged as open-source software that to make researchers' task more straightforward and easier.^[
For example, see `readtext` [@Benoit2021] for formatting text files and `DCPOtools` [@Solt2018] for aggregating cross-sectional time-series public-opinion surveys.]
<!-- "Write code instead of working by hand," as @Christensen2019 [, 197] admonish, "don't use Microsoft Excel if it can be avoided." -->

Second, __use the double-entry method__: when manual data entry cannot be avoided, each entry should be made twice.
Double entry is labor intensive, but experiments have shown that it reduces error rates by thirty-fold even when done immediately after the initial collection and by the same person [@Barchard2011, 1837].
Given that data-entry errors can completely undermine the validity of our conclusions, as in the example above, double entry is worth the extra effort.

Third, __embrace teamwork__: for any project involving entering data by hand, splitting the task up among team members will reduce the risk of errors going undetected.
When double entries are performed by different people, discrepancies will be noted, discussed, and resolved correctly; having two sets of eyes on complex materials like survey codebooks also increases the chances that nuances of the presentation like survey weights will be uncovered.
Further, by dividing the load, teamwork also lessens the probability of errors due to fatigue arising in the first place.

Fourth, __be aware of the threat of data-entry error__: this final recommendation is especially for manuscript reviewers.
If data-entry errors are invisible to the authors themselves, they are doubly so to reviewers (though if editors provided reviewers with replication materials at the time of the review it may help them to better assess the work's credibility).
But the case described above nevertheless suggests a valuable heuristic: when a work's conclusions suggest that a difficult problem will be easily solved---that democratic erosion will reflexively trigger a backlash and a renewed public support for democracy, in the present instance---it warrants especially careful scrutiny.

Data-entry errors are inevitable, and even following these recommendations is unlikely to eliminate them entirely.
Further, the above suggestions follow closely from a specific case and, although they successfully help us identify and fix its data-entry issues, they do not constitute a panacea to cure all data-processing problems in all types of research. 

Nonetheless, we also hope the readers to see the shared logic of these suggestions and the growing literature to guide political scientists to conduct more reliable and credible research.
For instance, in the same vein as our first suggestion, @Weidmann2023 provides a book-length set of illustrations on how to reduce "manual point and click" tasks found in a variety of studies with the `tidy`-data framework in the R language.
@KapiszewskiKarcher2021 [, 288] even suggests that qualitative researchers should consider using "open-exchange format" of qualitative data analysis software to be more "transparent about the generation and analysis of data."
Furthermore, we regard our efforts and recommendations as a contribution to the open science movement to produce more robust and credible research in the social sciences [see, e.g., @ChristensenEtAl2019] and beyond [see, e.g., @BarchardPace2011; @Lohr2014].
With careful attention, not only can the threat of data-entry errors to our 'janitor work', our research, and our understanding of the world be minimized, but the transparency, openness, and credibility of our research can continuously grow.

## References

::: {#refs}
:::

\pagebreak

\newpage
\appendix

# Online Supplementary Materials
\captionsetup[figure]{list=yes}
\setcounter{page}{1}
\renewcommand{\thepage}{SI-\arabic{page}}
\setcounter{figure}{0}
\renewcommand{\thefigure}{SI.\arabic{figure}}
\setcounter{table}{0}
\renewcommand{\thetable}{SI.\arabic{table}}
\listoftables
\listoffigures

# Tabular Results
\newpage
\blandscape
```{r num-labelAPSR, eval=FALSE}
names_coef <- c(
  "(Intercept)" = "(Intercept)",
  "plm::lag(SupDem_trim, 1:2)1" = "Democratic Mood (t-1)",
  "plm::lag(SupDem_trim, 1:2)2" = "Democratic Mood (t-2)",
  "diff(Libdem_z, lag = 1)" = "Liberal Democracy (Difference)",
  "Libdem_z" = "Liberal Democracy (Difference)",
  "plm::lag(Libdem_z, 1)" = "Liberal Democracy (t-1)",
  "diff(Polyarchy_z, lag = 1)" = "Electoral Democracy (Difference)",
  "Polyarchy_z" = "Electoral Democracy (Difference)",
  "plm::lag(Polyarchy_z, 1)" = "Electoral Democracy (t-1)",
  "diff(Liberal_z, lag = 1)" = "Minoritarian Democracy (Difference)",
  "Liberal_z" = "Minoritarian Democracy (Difference)",
  "plm::lag(Liberal_z, 1)" = "Minoritarian Democracy (t-1)",
  "diff(lnGDP_imp, lag = 1)" = "Log GDP Per Capita (Difference)",
  "lnGDP_imp" = "Log GDP Per Capita (Difference)",
  "plm::lag(lnGDP_imp, 1)" = "Log GDP (t-1)",
  "diff(Corrup_TI_z, lag = 1)" = "Corruption (Difference)",
  "Corrup_TI_z" = "Corruption (Difference)",
  "plm::lag(Corrup_TI_z, 1)" = "Corruption (t-1)"
)

names_gof <- tibble::tribble(
         ~raw,           ~clean, ~fmt,
       "nobs", "N observations",    0,
  "n.country",    "N countries",    0,
     "n.inst",  "N instruments",    0
  )
```
```{r tabulatingFun-APSR, eval=FALSE}
## function to extract statistics of MOC models
extract_statsAPSR <- function(input_data){
    # Reproducing the chunk `pointAPSR` with updated data
    sd.plm <- pdata.frame(input_data, index = c("country", "year")) 
    
    ls_ivECM <- c("diff(Libdem_z, lag = 1) + plm::lag(Libdem_z, 1)", 
                  "diff(Polyarchy_z, lag = 1) + plm::lag(Polyarchy_z, 1) + diff(Liberal_z, lag = 1) + plm::lag(Liberal_z, 1)")
    ls_ctrlECM <- c("", " + diff(Corrup_TI_z, lag = 1) + plm::lag(Corrup_TI_z, 1)")
    
    ls_eqECM <- glue("diff(SupDem_trim, lag = 1) ~ plm::lag(SupDem_trim, 1:2) + {ls_ivECM} + diff(lnGDP_imp, lag = 1) + plm::lag(lnGDP_imp, 1)") %>% 
      outer(ls_ctrlECM, paste0) %>% 
      as.vector()
    
    ls_ivFD <- c("Libdem_z",
                 "Polyarchy_z + Liberal_z")
    
    ls_ctrlFD <- c("", " + Corrup_TI_z")
    
    ls_eqFD <- glue("SupDem_trim ~ {ls_ivFD} + lnGDP_imp") %>% 
      outer(ls_ctrlFD, paste0) %>% 
      as.vector()
    
    ls_mod <- c(
      glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm)"),
      glue("plm({ls_eqFD}, model = 'fd', index = 'country', data = sd.plm)")
    )
    
    df_result_clsAPSR <- map(ls_mod, function(mod) {
      result <- eval(parse(text = mod))
      glance_result <- glance.plm(result)
      return(glance_result)
    })
    
    names(df_result_clsAPSR) <- c("pooled1", "pooled-regime1", "pooled2", "pooled-regime2", "fd1", "fd-regime1", "fd2", "fd-regime2")
    
    return(df_result_clsAPSR)
}

# function to produce data formated to fit `modelsummary`
format_resultAPSR <- function(condition, input_data){
    tidy_mocAPSR1 <-
        filter(result_APSR1, submodel == condition)
    tidy_mocRegAPSR1 <-
        filter(result_RegAPSR1, submodel == condition)
    tidy_mocAPSR2 <-
        filter(result_APSR2, submodel == condition)
    tidy_mocRegAPSR2 <-
        filter(result_RegAPSR2, submodel == condition)
    
    tidy_mocAPSR <-
        bind_rows(tidy_mocAPSR1,
                  tidy_mocRegAPSR1,
                  tidy_mocAPSR2,
                  tidy_mocRegAPSR2)
    
    vec_names <- levels(tidy_mocAPSR$model)
    
    tidy_mocAPSR <- group_split(tidy_mocAPSR, model)
    names(tidy_mocAPSR) <- vec_names
    tidy_mocAPSR <-
        tidy_mocAPSR[c(
            "Model 1.1 (ECM)",
            "Model 2.1 (ECM)",
            "Model 1.3 (FD)",
            "Model 2.3 (FD)",
            "Model 1.2 (ECM)",
            "Model 2.2 (ECM)",
            "Model 1.4 (FD)",
            "Model 2.4 (FD)"
        )]
    
    glance_mocAPSR <- extract_statsAPSR(input_data)
    glance_mocAPSR <-
        glance_mocAPSR[c(
            "pooled1",
            "pooled-regime1",
            "fd1",
            "fd-regime1",
            "pooled2",
            "pooled-regime2",
            "fd2",
            "fd-regime2"
        )]
    
    tb_pureAPSR <- map2(tidy_mocAPSR, glance_mocAPSR, ~ {
        result <- list(tidy = .x,
                       glance = .y)
        class(result) <- "modelsummary_list"
        return(result)
    })
    
    names(tb_pureAPSR) <-
        c(
            "ECM",
            "ECM-Regime",
            "FD",
            "FD-Regime",
            "ECM Corrup",
            "ECM Corrup-Regime",
            "FD Corrup",
            "FD Corrup-Regime"
        ) 
    
    return(tb_pureAPSR)
}
```
```{r num-pointAPSR, results='asis', eval=FALSE}
tb_result_clsAPSR <- map(result_clsAPSR_tidy, ~ {
  names(.) <- c("tidy", "glance")
  class(.) <- "modelsummary_list"
  return(.)}) 

names(tb_result_clsAPSR) <- unique(txt_model_lab) %>% rep(times = 2)

modelsummary(tb_result_clsAPSR[1:8], 
             stars = TRUE,
             coef_map = names_coef,
             gof_map = names_gof, 
             coef_omit = "Int",
             output = "latex", 
             title = "The Effect of Democracy on Change in Public Support (Publication Data)") %>% 
  kableExtra::kable_styling(font_size = 7) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")
```
```{r num-pointAPSRcorrected, results='asis', eval=FALSE}
modelsummary(tb_result_clsAPSR[9:16], 
             stars = TRUE,
             coef_map = names_coef,
             gof_map = names_gof, 
             coef_omit = "Int",
             output = "latex", 
             title = "The Effect of Democracy on Change in Public Support (Corrected Data)") %>% 
  kableExtra::kable_styling(font_size = 7) %>% 
  kableExtra::kable_styling(latex_options = "HOLD_position")
```
\elandscape

# Calculating Effects Via Simulation

As in @Claassen2020b [, 48-50] we estimate the effects of changes in democracy on public support for democracy in the error-correction models using simulation [see @Williams2012].
All independent variables were set to the same moderate values as in @Claassen2020b and allowed to run for 200 years, long enough for the system of equations to stabilize.
The level of democracy was then increased from half a standard deviation below the mean to half a standard deviation above; then the system of equations was allowed to run for 30 more years; these three decades those are depicted in Figure \@ref(fig:differencePlots).
Per @Claassen2020b [, Supplementary Information 3] and @Claassen2020c, the uncertainty in the model was captured by taking 10,000 draws from a multivariate normal distribution with expectation being the vector of model coefficients and variance being the robust covariance matrix, $\tilde{\Theta} ∼ MVN(\Theta, \Sigma)$, and adding the noise estimated in the regression standard error, $\tilde{Y}_i ∼ N(X_k \tilde{\Theta}_{ki}, \sigma)$.
To get first differences, the mean value of $\tilde{Y}_i$ in the year before the increase in democracy ($t = -1$) was subtracted from each $\tilde{Y}_i$, and the 0.025 and 0.975 quantiles of the first difference were used as its lower and upper confidence bounds.

\newpage
## First Difference Plots for Models 2.1 and 2.2
```{r firstDifferencePlots2, fig.cap= "Simulated Effects of Change in Minoritarian Democracy on Public Support", fig.width=6, fig.height=4, eval=FALSE}
txt_caption <- strwrap("Notes: Simulated effects are estimated using coefficients from the models presented in Figure 2 with corrected data. The solid lines indicate the mean simulated effect; the shaded regions indicate the 95% confidence intervals of these effects.", width = 90) %>% 
  paste0(sep="", collapse="\n")

ggplot(data = bind_rows(sim_data21, sim_data22)) +
  geom_hline(aes(yintercept = 0), colour="gray50", linetype="dashed") +
  geom_line(aes(x = year, y = fd)) +
  geom_ribbon(aes(x = year, ymin = l95, ymax = u95, alpha = .6)) +
  facet_wrap(~ model, ncol = 2) +
  scale_y_continuous(position = "right") +
  theme(strip.background = element_rect(colour="white", fill="white"),
        legend.position = "none",
        plot.caption.position = "plot",
        plot.caption = element_text(size = 10, hjust = 0)) +
  ylab(NULL) +
  xlab("Year") +
  ggtitle("Change in Public Democratic Support:\n1 SD Increase in Minoritarian Democracy at Year 0") +
  labs(caption = txt_caption)
```

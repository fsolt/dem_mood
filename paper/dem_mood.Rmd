---
output: 
  bookdown::pdf_document2:
    fig_caption: yes
    keep_tex: yes
    toc: no
    number_sections: yes
    latex_engine: xelatex
    # pandoc_args: --lua-filter=multiple-bibliographies.lua
    
title: "Democratic Support as Thermostatic Opinion? A Correction"
thanks: "Corresponding author: [yuehong-tai@uiowa.edu](mailto:yuehong-tai@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`."

# author:
# - name: Yue Hu
#   affiliation: Tsinghua University
# - name: Yuehong 'Cassandra' Tai
#   affiliation: University of Iowa
# - name: Frederick Solt
#   affiliation: University of Iowa
  
abstract: ""
keywords: ""
editor_options: 
  markdown: 
    wrap: sentence
tables: true # enable longtable and booktabs
citation_package: natbib
fontsize: 12pt
indent: true
linestretch: 1.5 # double spacing using linestretch 1.5
bibliography: dcpo_demsupport_text.bib
# \dummy{`r file.path(getwd(), list.files(getwd(), ".bib$", recursive = TRUE))`}
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/american-political-science-association.csl
biblio-style: apsr
citecolor: black
linkcolor: black
endnote: no
header-includes:
      - \usepackage{array}
      - \usepackage{caption}
      - \usepackage{graphicx}
      - \usepackage{siunitx}
      - \usepackage{colortbl}
      - \usepackage{multirow}
      - \usepackage{hhline}
      - \usepackage{calc}
      - \usepackage{tabularx}
      - \usepackage{threeparttable}
      - \usepackage{wrapfig}
      - \usepackage{fullpage}
      - \usepackage{pdflscape} #\usepackage{lscape} better for printing, page displayed vertically, content in landscape mode, \usepackage{pdflscape} better for screen, page displayed horizontally, content in landscape mode
      - \newcommand{\blandscape}{\begin{landscape}}
      - \newcommand{\elandscape}{\end{landscape}}
      - \usepackage{titlesec}
      - \titleformat*{\section}{\normalsize\bfseries}
      - \titleformat*{\subsection}{\normalsize\itshape}
---

Currently from dcpo_dem_mood/paper/dcpo_demsupport_appendix.Rmd; dataset replication code in dcpo_dem_mood/paper/dcpo_demsupport.Rmd may be better

needs dataset comparison from R/compare_supdem_cri.R


```{r setup, include=FALSE}
options(tinytex.verbose = TRUE)

knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE,
    dpi = 300
)

# If you haven't install `DCPOtools`
# remotes::install_github("fsolt/DCPOtools")

if (!require(pacman))
    install.packages("pacman")
library(pacman)

# p_load_gh("fsolt/DCPOtools",
#           "ropensci/osfr")

p_load(
    # data scraping
    dataverse,
    
    # analysis
    plm,
    
    # presenting
    patchwork,
    dotwhisker,
    kableExtra,
    modelsummary,
    ## tabulation
    latex2exp,
    patchwork,
    #rstan, # Bayesian estimation
    
    # munging
    glue,
    here,
    tidyverse # data wrangling
)

theme_set(theme_bw())

# Functions preload
set.seed(313)
source(here("R", "customizedFunctions.R"))

# Data preload
df_compare <- readRDS(here("data", "data_comparison.rds"))
df_correlation <- readRDS(here("data", "data_correlation.rds"))

df_apsr <- readRDS(here("data", "dem_mood_apsr.rds"))
load(here("data", "correct_cls_apsr.rda"))
df_apsrCorrected <- correct_cls_apsr %>% 
    rename(Country = country, Year = year)

df_plot <- arrange(df_correlation) %>%
    rownames_to_column() %>% 
    mutate(country = factor(country, levels = rev(country)),
           rowname = as.integer(rowname))

# dotwhisker::small_multiple hacks (showing the labels with latex signs)

body(small_multiple)[[19]] <- substitute(
    p <-
        ggplot(
            df,
            aes(
                y = estimate,
                ymin = conf.low,
                ymax = conf.high,
                x = as.factor(model),
                colour = submodel
            )
        ) +
        do.call(geom_pointrange, point_args) +
        ylab("") + xlab("") +
        facet_grid(
            term ~ .,
            scales = "free_y",
            switch = "y",
            labeller = label_parsed # enable the fancy label
        ) +
        scale_y_continuous(position = "right")
    
)
```


# DGP Problem and Consequences

We illustrate above DGP problems and their potential consequences with @Claassen2020b. 
The study published in a very prestige journal of political science with clear replication requirements.
Similar or relative data were also used in publications in other top journals of the field.
We appreciate the author's replication materials to enable the this scientific scrutiny.
Based on them and the author's description in the paper, we can largely infer how the measurements of variables are constructed.
We apply the same methods on the full available data with consistent coding.
By comparing the results with the original paper, we identify two primary problems of DGP, data discrepancy and coding inconsistency, which lead to results implying substantively different conclusions from the original publications.
We tend to use this case to show that the current consensus of publication replication for sure progress the scientificness of political science, whereas it does not prevent research from DGP problems and that may lead to severe consequences.

## Identification of Data and Coding Problems

The problem of data discrepancy refers to the practice that researchers consciously or unconsciously select to use only a part of data that are available.
When hypothesis tests rely on statistical estimates, both frequentist and Bayesian methodologists have emphasized the importance of the sufficiency of qualified data for producing unbiased and efficient estimates [XXX].
Empirical studies have also well documented that the insufficient use of available data would cause lethal misunderstanding of data trends and unreliable conclusions [@SoltEtAl2016; @SoltEtAl2017].

In our illustrative case, the 2020 paper, the point of interest is the influence of the institutional democracy on people's support of democracy, i.e., "democracy mood."
To measure the mood, the author uses existing survey questions about the "appropriateness or desirability of democracy, compare democracy to some
undemocratic alternative, or evaluate one of these undemocratic forms of government" to draw a latent variable with a dynamic IRT method [@Claassen2020b, p.40].
After the DGP, he collected 3,768 nationally aggregated opinions from 52 different survey questions of 14 survey projects. 
We reproduced the same process one year after the paper was published (2021) and found `r scales::percent((nrow(df_apsrCorrected) - nrow(df_apsr))/ nrow(df_apsr), accuracy = 0.1)` more data. 
For instance, the original study excludes observations from the third and fourth waves of Asian Barometers (the data were released in 2009 and 2017) on questions about to what extent people want their country to be democratic now, although it included them from the first and second waves. 
This accounts for 19 of 38 excluded country-year-items in 'available'.

The miscoding problem is caused by researchers' inconsistent coding.
Here we are not talking about the coding manipulations for "p-hiking" or "p-fishing," but that scholars apply invalid or variant coding method on the data that should be coded consistently [XXXX].
The problems easily occur when scholars intend to use multiple measurements for the same variable.
In @SoltEtAl2016, the authors show that how three seemingly valid measurements produce considerably different outcome estimates [-@SoltEtAl2016, p.4].
Howerver, the same problem can also occur even when only one measurement is used.

There are two potential miscoding types.
One is operation-based miscoding.
Researchers may mistakenly coded the values reversely, incompletely, or recognizing the value to mark missing response to a true variable value.
For instance, we found in @Claassen2020b that the author coded the option "Necesitamos un líder fuerte que no tenga que ser elegid" as "Strong_lapop_1" for the question in AmericasBarometer: 

>AUT1:Hay gente que dice que necesitamos un líder fuerte que no tenga que ser elegido a través del voto. AUT1 Otros dicen que aunque las cosas no funcionen, la democracia electoral, o sea el voto popular, es siempre
lo mejor. ¿Qué piensa usted? [Leer alternativas]

However, according to the publication's supplementary materials, "Strong_lapop_1" refers to the question in AmericasBarometer with wording, "On some occasions, democracy doesn't work. When that happens there are people that say we need a strong leader who doesn't have to be elected through voting. Others say that even if things don't function, democracy is always the best. What do you think?". 
"Strong_lapop_2" refers to the question with wording "There are people who say we need a strong leader who does not have to be elected. Others say that although things may not work, electoral democracy, or the popular vote,is always the best. What do you think?"
Therefore, this question should be coded as "Strong_lapop_2" instead of "Strong_lapop_1".

The other type of problem is design-based mis(re)coding.
Researchers conduct consistent method during the coding but the method per se under-represent or stretch the variance of the variables.
For examples, when recoding the questions about how people evaluate democracy ^[The question wording is "I’m going to describe various types of political systems. Please indicate for each system whether you think it would be very good, fairly good or bad for this country – A democratic political system"] in the Asia Barometer, @Claassen2020b counted the midpoint of its three-point scale as a positive, democracy-supporting response.
The coding results in substantial overreports in 35 country-years.
Similarly, in the *second* wave of the Asian Barometer, the study counts 5 along with 6 through 10 as positive responses for questions about the suitability of democracy^[The question is "Here is a similar scale of 1 to 10 measuring the extent to which people think democracy is suitable for our country. If 1 means that democracy is completely unsuitable for [name of country] today and 10 means that it is completely suitable, where would you place our country today?"] and desire for democracy^[The questions is "To what extent do you want our country to be democratic now?"].
This coding produces more modest overreports in 9 country-years.
A third example in recoding Pew Global Attitudes surveys, the researcher counted only the highest value of the four-point scale of the question on how important to live in a courntry with regular elections ^[The question wording is "How important is it to you to live in a country where honest elections are held regularly with a choice of at least two political parties? Is it very important, somewhate important, not too important or not important at all?"] as a positive response and so leading to substantial underreports in 91 country-years.  
A final example we found that the 2020 study counts nonresponses as negative responses (such as in Pakistan in 2005 in the South Asian Barometer, when 636 of 1324 respondents declined to answer whether they considered democracy suitable for their country, and these were all coded as unsupportive responses) yielding under-reports. 
Moreover, the study does not always employ survey weights, which can shift proportions somewhat in either direction.

Both the data-discrepancy problem and design-based mis(re)coding are covert DGP problems that are often difficult to be detected merely through reproductions of the replication files. 
However, the discrepancy they cause could be substantial.
We compare the distributions of the core explanatory variable, democracy support, from the replication data of the publication and a version that fixed all the problems discussed above at both country and survey levels.
The overall correlation of the mean estimates seems tolerable as `r round(cor(df_compare$prop_ours, df_compare$prop_cls), digits = 2)`, but as Figure \@ref(fig:comparison1) shows over one third of countries have lower correlations. 
The minimum correlation is `r round(min(df_plot$corr), digits = 2)`.

The correlation within the time series of individual countries can be much lower, and it is even negative for 15 countries.[HOW TO PRESENT THEEVIDENCE?]

In Figure \@ref(fig:compare2), we compared democracy support from the publication and corrected data across surveys^[afrob:Afrobarometer, amb:AmericasBarometer, arabb:Arab Barometer, asiab:AsiaBarometer, asianb:Asian Barometer, cses:Comparative Study of Electoral Systems, eb:Eurobarometer, ess:European Social Survey, evs:European Value Survey, lb:Latinobarómetro, neb:New Democracies Barometer, pew:Pew Global Attitudes Survey, sasianb:South Asian Barometer, wvs:World Values Survey.]. 
The extent the points in the plot deviating from the 45&deg; diagonal represents the discrepancy of the publication data from the full, corrected-coding data.
The comparison witnesses a large amount of data from various surveys, especially from Pew Global Attitudes Survey, were underreported and a fair amount of data, mainly from the Asia barometers, were overreported.


In the next section, we illustrate how these DGP errors causes unreliable conclusion for substantive research question.

\newpage
\blandscape

```{r comparison1, fig.cap="Country-Level Comparision of Support Democracy Responses in the Publication and Corrected Data", fig.align='center', fig.height=8, fig.width=9}
vec_bar <- nrow(df_plot) - min(df_plot$rowname[df_plot$corr >= cor(df_compare$prop_ours, df_compare$prop_cls)]) # where the line is drawn
                               
plot_corr <- ggplot(df_plot, aes(y = country, x = corr)) +
        geom_point() +
        guides(y = guide_axis(n.dodge = 2)) +
    xlab("Correlation") +
    geom_hline(yintercept = vec_bar,
               colour = "grey60", linetype = 2) +
    geom_text(aes(0.75, vec_bar),
              label = paste0("Cor. = ", round(cor(df_compare$prop_ours, df_compare$prop_cls), digits = 2)), vjust = -1) +
    theme(axis.title.y = element_blank())

ggsave('images/data_comparison2.png') # Stand-alone plot without notes for presentation


tx_note <- str_wrap(
    c(
      " Source: Claassen 2020 and self-collected data"
    ),
    width = 115
  )

plot_corr + 
    plot_annotation(caption = tx_note) &
    theme(plot.caption = element_text(hjust = 0, face= "italic"))
```

```{r comparison2, fig.cap="Survey Comparision of Support Democracy Responses in the Publication and Corrected Data", fig.align='center', fig.height=8, fig.width=9}
plot_support <- ggplot(df_compare, 
       aes(x = prop_ours, y = prop_cls)) +
    geom_point(aes(color = p_dcpo, shape = p_dcpo), alpha = 0.5) +
    labs(y = "Publication Data",
         x = "Corrected Data") +
    annotate("text",
             x = .22,
             y = .95,
             label = "evdemoc_asiab accounts\nfor most overreports") +
    annotate("text",
             x = .75,
             y = .13,
             label = "impdemoc_pew accounts for\nmost large underreports") +
    scale_color_discrete(name = "Survey", direction = -1) +
    scale_shape_manual(values = seq(1:length(unique(df_compare$p_dcpo))), 
                       name = "Survey")

ggsave('images/data_comparison1.png') # Stand-alone plot without notes for presentation

tx_note <- str_wrap(
    c(
      " Source: Claassen 2020 and self-collected data"
    ),
    width = 115
  )

plot_support + 
    plot_annotation(caption = tx_note) &
    theme(plot.caption = element_text(hjust = 0, face= "italic"))
```


\elandscape

## Consequence of DGP Problem

@Claassen2020b claimed a vital challenge against the existing literature of public support of democracy and common knowledge with a negative thermostatic mechanism that the enhancement of institutional democracy depresses the support of the government (or "democracy mood"), whereas the reduction cheer it. 
The author also argued that there is also a regime relativity of the effect that it affects the minoritarian rather than electoral aspect of democracy.
In the empirical part, the study used an error-correction model (EC) to test the relationship between the changes of the liberal-democracy degree of a country and the then-moment public support together with a model including the dynamics of public corruption perception and a first-difference model (FD) as robustness checks.
The results show a negative effect of the institutional-democracy changes on the public democracy support with statistical significant at the 0.05 level in both EC and FD [@Claassen2020b, p.47 and Table 1].
Moreover, the author built up separated measurements for the minoritarian and electoral aspects of democracy and included them into the original model.
The empirical result support the author's argument that the minoritarian measurement remains statistically significant but not the electoral one [@Claassen2020b, p.49 and Table 2].

We replicate these findings with corrected data and parallel them together with original results.
We include all the explanatory and control variables and use exactly the same models structures and software to conduct the analysis.
Figure \@ref(fig:regressionPlot) presents the results with a "small_multiple" plot for a clear comparison of each variable across models [@SoltHu2015]. 
In the plot, the dots are point estimates and the whiskers represent 95% confidence intervals.
The light dot-whiskers are from the original work, and the dark ones are the estimates from the corrected data.
Each facet row represents a variable's performance in its own scale across models.

```{r regressionResult}
sd.plm <- pdata.frame(df_apsr, index = c("Country", "Year")) 
sd.plm2 <- pdata.frame(df_apsrCorrected, index = c("Country", "Year"))

ls_ivECM <- c("diff(Libdem_z, lag = 1) + plm::lag(Libdem_z, 1)", 
              "diff(Polyarchy_z, lag = 1) + plm::lag(Polyarchy_z, 1) + diff(Liberal_z, lag = 1) + plm::lag(Liberal_z, 1)")
ls_ctrlECM <- c("", " + diff(Corrup_TI_z, lag = 1) + plm::lag(Corrup_TI_z, 1)")

ls_eqECM <- glue("diff(SupDem_trim, lag = 1) ~ plm::lag(SupDem_trim, 1:2) + {ls_ivECM} + diff(lnGDP_imp, lag = 1) + plm::lag(lnGDP_imp, 1)") %>% 
  outer(ls_ctrlECM, paste0) %>% 
  as.vector()

ls_ivFD <- c("Libdem_z",
             "Polyarchy_z + Liberal_z")

ls_ctrlFD <- c("", " + Corrup_TI_z")

ls_eqFD <- glue("SupDem_trim ~ {ls_ivFD} + lnGDP_imp") %>% 
  outer(ls_ctrlFD, paste0) %>% 
  as.vector()

ls_mod_original <- c(
  glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm)"),
  glue("plm({ls_eqFD}, model = 'fd', index = 'Country', data = sd.plm)")
)
ls_mod_correct <- c(
  glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm2)"),
  glue("plm({ls_eqFD}, model = 'fd', index = 'Country', data = sd.plm2)")
)

ls_mod <- c(ls_mod_original, ls_mod_correct)

result_clsAPSR <- map(ls_mod, ~eval(parse(text = .)))

result_clsAPSR_tidy <- map(result_clsAPSR, function(aResult) {
  tidy_result <- tidy(aResult, conf.int = TRUE) %>% 
    mutate(std.error = vcovHC_se(aResult))
  
  glance_result <- glance.plm(aResult)
  
  ls_result <- list(tidy_result, glance_result)
})
```

```{r regressionPlot, fig.cap= "The Effect of Democracy on Change in Public Support with Uncertainty", fig.width=7, fig.height=10}

txt_models <- paste0("Model ",
                     rep(1:2, each = 4), ".", rep(1:4, 2),
                     " (",
                     rep(c("EC", "EC", "FD", "FD"), 2),
                     ")")

vec_numCoef <- map_dbl(c(1, 2, 5, 6, 3, 4, 7, 8), # ECM ECM FD FD
                       ~nrow(result_clsAPSR_tidy[[.]][[1]])) - 1 # subtract the intercepts

txt_model_lab <- map2(txt_models, vec_numCoef, ~rep(.x, times = .y)) %>% 
    unlist # model lists
txt_model_lab <- c(txt_model_lab[str_which(txt_model_lab, "EC")], 
                   txt_model_lab[str_which(txt_model_lab, "FD")])
# Modify the orders to match the dataset

index_coefName <- c(
    `plm::lag(SupDem_trim, 1:2)1` = "Democratic~Support[t-1]",
    `plm::lag(SupDem_trim, 1:2)2` = "Democratic~Support[t-2]",
    `diff(Libdem_z, lag = 1)` = "Delta~Liberal~democracy",
    `Libdem_z` = "Delta~Liberal~democracy",
    `plm::lag(Libdem_z, 1)` = "Liberal~Democracy[t-1]",
    `diff(Polyarchy_z, lag = 1)` = "Delta~Electoral~Democracy",
    `Polyarchy_z` = "Delta~Electoral~Democracy",
    `plm::lag(Polyarchy_z, 1)` = "Electoral~Democracy[t-1]",
    `diff(Liberal_z, lag = 1)` = "Delta~Minoritarian~Democracy",
    `Liberal_z` = "Delta~Minoritarian~Democracy",
    `plm::lag(Liberal_z, 1)` = "Minoritarian~Democracy[t-1]",
    `diff(lnGDP_imp, lag = 1)` = "Delta~Log~GDP~per~capita",
    `lnGDP_imp` = "Delta~Log~GDP~per~capita",
    `plm::lag(lnGDP_imp, 1)` = "Log~GDP~per~capita[t-1]",
    `diff(Corrup_TI_z, lag = 1)` = "Delta~Corruption",
    `Corrup_TI_z` = "Delta~Corruption",
    `plm::lag(Corrup_TI_z, 1)` = "Corruption[t-1]"
)

df_plot <- map_dfr(result_clsAPSR_tidy, ~.[[1]]) %>% 
    filter(term != "(Intercept)") %>%
    mutate(
        type = ifelse(str_detect(term, "(_z|imp)$"), "FD", "EC"),
        term = index_coefName[term],
        term = factor(term, levels = unique(index_coefName)),
        submodel = rep(c("Published", "Corrected"), each = nrow(.)/2),
        submodel = factor(submodel, levels = c("Published","Corrected")),
        model = rep(txt_model_lab, times = 2),
        model = factor(model, levels = txt_models)
    )

txt_caption <- strwrap("Note: Replications of Claassen (2020), Table 1, 47, and Table 2, 49.  Models denoted 'EC' are error-correction models; those marked 'FD' are first-difference models.", width = 85) %>% 
  paste0(sep="", collapse="\n")

small_multiple(df_plot, axis_switch = TRUE, dot_args = list(size = .9, fatten = 1)) +
    geom_hline(yintercept = 0, colour = "grey50", linetype = 2) +
    scale_color_grey(start = 0.8, end = 0.4, 
                     name = "Replication",
                     breaks = c("Published",
                              "Corrected"),
                     labels = c("Publication data",
                              "Corrected data")) +
    theme(axis.text.x  = element_text(angle = 90, hjust = .5),
          strip.text.y.left = element_text(angle = 0),
          legend.position = c(0.006, -.01),
          legend.justification = c(1, 1), 
          legend.title = element_text(size = 9, face = "bold"),
          legend.title.align = 0.5,
          legend.text = element_text(size = 8),
          legend.background = element_rect(color="gray90"),
          legend.spacing = unit(-5, "pt"),
          legend.key.width = unit(4.5, "mm"),
          legend.key.height = unit(7, "mm"),
          plot.caption = element_text(size = 10, hjust = 0.2, margin = margin(t=15))) +
  guides(color = guide_legend(override.aes = list(size=.3))) +
  ggtitle("Dependent Variable: Change in Public Democratic Support") +
  labs(caption = txt_caption)

```

According to the plot, the consequence of DGP problem is evident.
In particular, the estimates of the core explanatory variable &Delta; `Liberal Democracy` are no longer statistically significant in none of the EC, EC with corruption perception, or FD model (Row 3, Figure \@ref(fig:regressionPlot)).
In other words, when the data is sufficient and corrected coded, it does not draw evidence to support the thermostatic effect. 
Similar changes are observe in the models with separate-democracy measures.
&Delta; `Minoritarian Democracy`&Delta; does not perform that different from `Electoral Democracy` as the theory expects (Row 7, Figure \@ref(fig:regressionPlot)).

# Discussion and Suggestions

With the previous sections, we identified two types of DGP problems, data discrepency and miscoding.
The former refers to the problem that not all available data are included in the analysis.
The latter relates to human or design errors to (re)code information from the original data source inconsistently or unvaildly. 
We use @Claassen2020b to illustrate these issues in practice and how correcting them would substantively change the conclusions of hypothesis tests. 

The DGP problems are often too convert to be detected merely through reproduction of the results, but they can lead to severe misunderstanding of the empirical outcomes of scientific inquiries.
A better way to deal with the DGP problems is more doing with the design than post-estimate phases.
Here we give researchers four suggestions to minimize DGP problems in their research.
First three are for the authors, and the last one is for the reviewers.

First, automatic downloading.
We suggest researchers to maximumly utilize the data scrapping functions of programming languages (such as R and Python) to collect data through with widecard searching and directly from the original sources.
For instance, you can easily use the package [`icpsrdata`](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwj9-56bu6jzAhWYGDQIHaG4DxkQFnoECAIQAQ&url=https%3A%2F%2Fwww.icpsr.umich.edu%2Fweb%2Fpages%2Fdatamanagement%2Fdmp%2Fplan.html&usg=AOvVaw37vSc0BTKmgmesx-1ZFrOC) to obtain all the latest data from ICPSR or use [`pewdata`] to access all types of data published by Pew.
Instead of manually updates, the programming way collect data directly from the datasets, which reduces the missing probability.

Second, automatic coding. 
We suggest researchers to process their data coding as automatically as possible. 
Instead of coding every variables with separate codes, building up packed-up functions or even packages for doing batch coding.
For instance, if you work with R, you can find functions such as `stm::textProcessor` or packages like [`DCPOtools`](https://github.com/fsolt/DCPOtools).
The former prepares raw text data for more complex text-analysis tasks, and the latter transfer different survey questions into a consistent format ready for analysis and comparison.
By this way, the authors can ensure every variable is treated in the same manner. 
Of course, the actual data cleaning process is complicated that maybe not all the steps can be packed into functions or batched as a process. 
In this scenario, we gives the following suggestion.

Third, replication from the scratch.
Given the complication of the data management, we suggest researchers to provide fully replication codes on the raw data.
Researchers ought to provide not only replication files that can reproduce the results in the publications but concrete coding programs or coding books that others can replicate the DGP process. 
This is an advanced requirement for data transparency and replicability.
Yet as we showed, there could be serious drawbacks to ignore this step, especially for studies based on existing datasets. 

Finally, we hope this replication can alert not only the researchers but also the reviewers of the importance of DGP. 
It is usually the very first empirical step for a research and a crucial step determining the validity of all the outcomes and conclusions. 
We appeal to both the academic journals and reviewers to pay more attention to this phase when assess a manuscript. 
The substantive difference elaborated in this paper demonstrate that the DGP problem is no less vital than reproduction or any other important methodological problems.


\pagebreak


# Reference
---
output: 
  bookdown::pdf_document2:
    fig_caption: yes
    keep_tex: yes
    toc: no
    number_sections: no
    latex_engine: xelatex
    # pandoc_args: --lua-filter=multiple-bibliographies.lua

title: |
  | On the Importance of
  | 'Janitor Work' in Political Science:
  | The Case of Thermostatic Support for Democracy
thanks: "Corresponding author: [yuehong-tai@uiowa.edu](mailto:yuehong-tai@uiowa.edu). Current version: `r format(Sys.time(), '%B %d, %Y')`."
author:
- Yue Hu
  # affiliation: Tsinghua University
- Yuehong 'Cassandra' Tai
  # affiliation: University of Iowa
- Frederick Solt
  # affiliation: University of Iowa
anonymous: false  
abstract: ""
keywords: ""
tables: true # enable longtable and booktabs
citation_package: natbib
fontsize: 12pt
indent: true
linestretch: 1.5 # double spacing using linestretch 1.5
bibliography: dem_mood_text.bib
# \dummy{`r file.path(getwd(), list.files(getwd(), ".bib$", recursive = TRUE))`}
# csl: https://raw.githubusercontent.com/citation-style-language/styles/master/american-political-science-association.csl
biblio-style: apsr
citecolor: black
linkcolor: black
endnote: no
header-includes:
      - \usepackage{array}
      - \usepackage{caption}
      - \usepackage{graphicx}
      - \usepackage{siunitx}
      - \usepackage{colortbl}
      - \usepackage{multirow}
      - \usepackage{hhline}
      - \usepackage{calc}
      - \usepackage{tabularx}
      - \usepackage{threeparttable}
      - \usepackage{wrapfig}
---

```{r setup, include=FALSE}
options(tinytex.verbose = TRUE)

knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE,
    dpi = 300
)

if (!require(pacman))
    install.packages("pacman")
library(pacman)

p_install(janitor, force = FALSE)
p_install_gh(c("fsolt/DCPOtools"), force = FALSE)

# load all the packages you will use below
p_load(
    # analysis
    plm,
    
    # presentation
    gridExtra,
    modelsummary,
    dotwhisker,
    latex2exp,
    
    # data wrangling
    DCPOtools,
    janitor,
    here,
    broom,
    tidyverse,
    glue)

# Functions preload
set.seed(313)

## analyzing and presenting functions
# Variance Calculation ------------------------------------------------------------------

## Beck-Katz panel-corrected standard errors
vcovHC_se <-  function(x) {
    plm::vcovHC(x, method="arellano", cluster="group") %>%  #default setting
        diag() %>% 
        sqrt()
}

# Tabulation -----------------------------------------------------------------------
na_types_dict <- list("r" = NA_real_,
                      "i" = rlang::na_int,
                      "c" = NA_character_,
                      "l" = rlang::na_lgl)

# A function that converts a string to a vector of NA types.
# e.g. "rri" -> c(NA_real_, NA_real_, rlang::na_int)
parse_na_types <- function(s) {
    
    positions <- purrr::map(
        stringr::str_split(s, pattern = ""), 
        match,
        table = names(na_types_dict)
    ) %>%
        unlist()
    
    na_types_dict[positions] %>%
        unlist() %>%
        unname()
}

# A function that, given named arguments, will make a one-row
# tibble, switching out NULLs for the appropriate NA type.
as_glance_tibble <- function(..., na_types) {
    
    cols <- list(...)
    
    if (length(cols) != stringr::str_length(na_types)) {
        stop(
            "The number of columns provided does not match the number of ",
            "column types provided."
        )
    }
    
    na_types_long <- parse_na_types(na_types)
    
    entries <- purrr::map2(cols, 
                           na_types_long, 
                           function(.x, .y) {if (length(.x) == 0) .y else .x})
    
    tibble::as_tibble_row(entries)
    
}

tidy.pgmm <- function(x,
                      conf.int = FALSE,
                      conf.level = 0.95,
                      ...) {
    result <- summary(x)$coefficients %>%
        tibble::as_tibble(rownames = "term") %>%
        dplyr::rename(
            estimate = Estimate,
            std.error = `Std. Error`,
            statistic = `z-value`,
            p.value = `Pr(>|z|)`
        )
    
    if (conf.int) {
        ci <- confint(x, level = conf.level) %>% 
            as.data.frame() %>% 
            rownames_to_column(var = "term") %>% 
            dplyr::rename(
                conf.low = `2.5 %`,
                conf.high = `97.5 %`
            )
        result <- dplyr::left_join(result, ci, by = "term")
    }
    
    result
}

glance.plm <- function(x, ...) {
    s <- summary(x)
    as_glance_tibble(
        nobs = stats::nobs(x),
        n.country = pdim(x)$nT$n,
        na_types = "ii"
    )
}

glance.pgmm <- function(x, ...) {
    s <- summary(x)
    as_glance_tibble(nobs = stats::nobs(x),
        n.country = pdim(x)$nT$n,
        n.inst = dim(x$W[[1]])[2],
        na_types = "iii"
    )
}

theme_set(theme_bw())

# Data preload
df_compare <- readRDS(here("data", "data_comparison.rds"))
# df_correlation <- readRDS(here("data", "data_correlation.rds"))
df_apsr <- rio::import(here("data", "dem_mood_apsr.rda"))
df_apsrCorrected <- rio::import(here("data", "correct_cls_apsr.rda")) %>% 
    rename(Country = country, Year = year)
# df_plot <- arrange(df_correlation) %>%
#     rownames_to_column() %>% 
#     mutate(country = factor(country, levels = rev(country)),
#            rowname = as.integer(rowname))

## dotwhisker::small_multiple() hacks
body(small_multiple)[[19]] <- substitute(
    p <-
        ggplot(
            df,
            aes(
                y = estimate,
                ymin = conf.low,
                ymax = conf.high,
                x = as.factor(model),
                colour = submodel
            )
        ) +
        do.call(geom_pointrange, point_args) +
        ylab("") + xlab("") +
        facet_grid(
            term ~ .,
            scales = "free",
            labeller = label_parsed,
            # enable LaTeX facet labels
            switch = "y"
        ) +             # put facet labels on left
        scale_y_continuous(position = "right") # put axis label on right
)


```

```{r data_comparison, include=FALSE}
if (!file.exists(here::here("data", "supdem raw survey marginals.tab"))) {
    tempfile <- dataverse::get_file("supdem raw survey marginals.tab", "doi:10.7910/DVN/HWLW0J") # AJPS replication file, not included in APSR replication
    
    writeBin(tempfile, here::here("data", "supdem raw survey marginals.tab"))
    rm(tempfile)
}

sd <- read_csv(here::here("data", "supdem raw survey marginals.tab"), col_types = "cdcddcdc") %>% 
    mutate(item_fam = str_extract(tolower(Item), "^[a-z]+"),
           country = countrycode::countrycode(Country, "country.name", "country.name"),
           dataset = "supdem") %>% 
    rename(year = Year, project = Project) %>% 
    with_min_yrs(2) # Selecting data w. at least two years

if (!file.exists(here::here("data", "claassen_input_raw.csv"))) {
claassen_input_raw <- DCPOtools:::claassen_setup(vars = read_csv(here::here("data-raw", "mood_dem.csv"),
                                                                 col_types = "cccccc"),
                                                 file = "data/claassen_input_raw.csv")
}

claassen_input_raw <- read_csv(here::here("data", "claassen_input_raw.csv"),
                               col_types = "cdcddcd")

claassen_input_raw1 <- claassen_input_raw %>%
    filter(!((
        str_detect(item, "army_wvs") &
            # WVS obs identified as problematic by Claassen
            ((country == "Albania" & year == 1998) |
                 (country == "Indonesia" &
                      (year == 2001 | year == 2006)) |
                 (country == "Iran" & year == 2000) |
                 (country == "Pakistan" &
                      (year == 1997 | year == 2001)) | # 1996 in Claassen
                 (country == "Vietnam" & year == 2001)
            )
    ) |
        (
            str_detect(item, "strong_wvs") &
                ((country == "Egypt" & year == 2012) |
                     (country == "Iran" &
                          (year == 2000 | year == 2007)) | # 2005 in Claassen
                     (country == "India") |
                     (country == "Pakistan" &
                          (year == 1997 | year == 2001)) | # 1996 in Claassen
                     (country == "Kyrgyzstan" &
                          (year == 2003 | year == 2011)) |
                     (country == "Romania" &
                          (year == 1998 | year == 2005 | year == 2012)) |
                     (country == "Vietnam" & year == 2001)
                )
        ) |
        (
            country %in% c(
                "Puerto Rico",
                "Northern Ireland",
                "SrpSka Republic",
                "Hong Kong SAR China"
            )
        ))) %>%
    with_min_yrs(2)

claassen_input <- DCPOtools:::format_claassen(claassen_input_raw1)

cri <- claassen_input$data %>% 
    mutate(p_dcpo = str_extract(survey, "^[a-z]+"), 
           project = case_when(p_dcpo == "afrob" ~ "afb",
                               p_dcpo == "amb" ~ "lapop",
                               p_dcpo == "arabb" ~ "arb",
                               p_dcpo == "asiab" ~ "asb",
                               p_dcpo == "asianb" ~ "asnb",
                               p_dcpo == "neb" ~ "ndb",
                               p_dcpo == "sasianb" ~ "sab",
                               TRUE ~ p_dcpo),
           item_fam = str_extract(item, "^[a-z]+"),
           item_fam = if_else(item_fam == "election", "elec", item_fam),
           dataset = "cri")

supdem_cri <- full_join(sd, cri, by = c("country", "year", "item_fam", "project"))

# While DCPO assigns actual year of survey, Claassen used nominal year of wave,
# so some corrections are required to match observations (~8% of obs)
no_problems <- inner_join(sd %>% select(-dataset), 
                          cri %>% select(-dataset),
                          by = c("country", "year", "item_fam", "project")) # 3409 obs

needed <- anti_join(sd %>% select(-dataset),
                    cri %>% select(-dataset))                   # 307 obs

available <- anti_join(cri %>% select(-dataset),
                       sd %>% select(-dataset))                # 1418 obs

year_fixes <- left_join(needed,    # 304 obs
                        available,
                        by = c("country", "project", "item_fam")) %>% 
  mutate(diff = year.x - year.y) %>% 
  group_by(country, project, item_fam, year.x) %>% 
  mutate(closest_to_claassen = min(abs(diff))) %>% 
  ungroup() %>% 
  group_by(country, project, item_fam, year.y) %>% 
  mutate(closest_to_dcpo = min(abs(diff))) %>% 
  ungroup() %>% 
  filter(closest_to_claassen == abs(diff) & closest_to_dcpo == abs(diff) & abs(diff) <= 3) %>% 
  filter(!(country == "Egypt" & year.x == 2014 & survey == "afrob5")) %>%  # double matches (it's really afrob6)
  mutate(year = year.y) %>% 
  select(names(no_problems))

data_comparison <- bind_rows(no_problems, year_fixes) %>% 
  mutate(perc_cls = Response/Sample*100,
         perc_ours = x/samp*100,
         diff_perc = perc_cls - perc_ours,
         diff_perc_abs = abs(diff_perc),
         diff_x = round(Response - x),
         diff_samp = round(Sample - samp)) %>% 
  select(-CAbb, -COWCode)

saveRDS(data_comparison, here("data", "data_comparison.rds"))
```

@Claassen2020c

Many facets of Computational social science 
Data wrangling, the task of getting the needed data into the format required to perform analyses.

[@Wickham2017, xi]

Such data 'janitor work' is often viewed as tiresome and as something to be delegated to research assistants---to someone, anyone, else [see @Torres2017].


# DGP Problem and Consequences



<!-- Maybe also something about the documentation by @HerndonAshPollin2014 of the problematic data selection in @ReinhartRogoff2010 -->

<!-- Related: @Troeger2019 [, 285] says, "Another potential measure is to make all data publicly available. Again many journals require data and code to be made available to the public before publication. But often there are no requirements whether source data has to be included. When source data is original, confidential, or personalized, publication might not be possible or undesirable. However, new avenues to make this kind of data available for replication need to be explored."  -->

## Data-Entry Errors: An Example

Claassen's coding rule

In Figure \@ref(fig:comparison), we compare the percentage of respondents to give a democracy-supporting response in the publication's replication data with the percentage we found when we automated the process of wrangling these same data from their original surveys.
When points fall along the 45&deg; diagonal, it indicates that the two data-wrangling processes yielded the same percentages.
Points above this diagonal represent observations for which the publication data overestimated the actual percentage of respondents who offered a democracy-supporting response, while points below this line are observations where the publication data underestimated this percentage.

```{r comparison, fig.cap="Comparing Democracy-Supporting Responses in the Publication Data and the Corrected Data", fig.align='center', fig.width=5.25, fig.height=5.6}
bad_items <- c("Asia Barometer:\nEvaluation of Democracy",
               "Asian Barometer II:\nSuitability of Democracy",
               "Pew Global Attitudes:\nImportance of Democracy",
               "Other Items")

plot_support <- data_comparison %>% 
  mutate(p_dcpo3 = case_when(Item == "EvDemoc_asb" ~ "Asia Barometer:\nEvaluation of Democracy",
                             Item == "DemocSuit_asnb" & between(year, 2005, 2008) ~ "Asian Barometer II:\nSuitability of Democracy",
                             Item == "ImpDemoc_pew" ~ "Pew Global Attitudes:\nImportance of Democracy",
                             TRUE ~ "Other Items") %>% 
           factor(levels = bad_items)) %>% 
  ggplot(aes(x = perc_ours, y = perc_cls)) +
    geom_point(aes(color = p_dcpo3, shape = p_dcpo3), size = 1.5) +
    labs(y = "Publication Data",
         x = "Corrected Data") +
  theme_bw() +
  theme(legend.justification=c(1,0), 
        legend.position=c(.99,.01),
        legend.text = element_text(size = 6),
        legend.background = element_rect(size=.1,
                                         linetype="solid",
                                         color = "black"),
        legend.key.size = unit(1.1, 'lines')) +
  scale_colour_manual(name = element_blank(),
                        values = alpha(c("blue", "green", "red", "gray50"),
                                       c(1, 1, 1, .3)),
                        breaks = bad_items) +
  scale_shape_discrete(name = element_blank(),
                       breaks = bad_items)

ggsave('images/data_comparison1.png') # Stand-alone plot without notes for presentation

tx_note <- str_wrap(
    c(
      "Notes: Each point represents the percentage of respondents in a country-year to give a democracy-supporting response to a particular survey item.  Publication data is as reported in Claassen (2020b); the corrected data was collected directly from the original surveys.  The Asia Barometer's item on the evaluation of democracy accounts for most overreports, and the Pew Global Attitudes item on the importance of democracy accounts for most substantial underreports.  In both cases, as well as the overreports of the suitability of democracy item in the second wave of the Asian Barometer, the issues can be easily explained by errors in transcribing the data.  Deviations in other items result from inconsistent treatment of missing data and/or survey weights, reflecting in part differences in codebook reporting practices across surveys."
    ),
    width = 90
  )

plot_support + 
    patchwork::plot_annotation(caption = tx_note) &
    theme(plot.caption = element_text(hjust = 0))

```

For `r {mean(data_comparison$diff_perc_abs < .5)*100} %>% round()`% of the country-year-item observations, the difference between these percentages was negligible---less than half a percent---yielding points approximately along the 45&deg; diagonal.
But for the remaining observations, as a result of data-entry errors, the difference was often substantial.
For example, the Asia Barometer asked respondents in 35 country-years to indicate whether they thought "a democratic political system" would be very good, fairly good, or bad for their country.
According the study's coding rules [see @Claassen2020b, Appendix 1.3], only answers above the median response category should be considered as democracy supporting, yet in this case the lukewarm intermediate category was coded as supporting democracy as well.^[
Although this may be interpreted as an exercise of researcher judgment as to what constitutes a democracy-supporting response rather than a data-entry error, examination of similar answers to similar questions shows that similarly lukewarm responses at and below the median response category  (e.g., in the Arab Barometer, that democracy was "somewhat appropriate" for the country) were coded as not supporting democracy.
]
This led to overestimations of the percentage of democracy-supporting responses ranging from `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% min() %>% round()` to `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% max() %>% round()` percentage points and averaging `r data_comparison %>% filter(item=="evdemoc_asiab") %>% pull(diff_perc) %>% mean() %>% round()` points.

Similarly, the four waves of the Asian Barometer included the following item: "Here is a similar scale of 1 to 10 measuring the extent to which people think democracy is suitable for our country. If 1 means that democracy is completely unsuitable for [name of country] today and 10 means that it is completely suitable, where would you place our country today?"
In accordance with the coding rules of the study, responses of 6 through 10 are considered democracy supporting, and that is how the first, third, and fourth waves of the survey are coded.
For the second wave, however, 5 was erroneously included.
This data-entry error resulted in overestimates of as much as `r data_comparison %>% filter(item=="democsuit_asianb" & (year==2005|year==2006|year==2008)) %>% pull(diff_perc) %>% max() %>% round(1)`% in 9 country-years. 

A third example comes from the Pew Global Attitudes surveys' four-point item asking about the importance of living in a country with regular and fair contested elections:
the question wording is "How important is it to you to live in a country where honest elections are held regularly with a choice of at least two political parties? Is it very important, somewhat important, not too important or not important at all?"
In this case, rather than including respondents who gave both responses above the median---"very important" and "somewhat important"---only those respondents who answered "very important" were entered as supporting democracy.
This error caused substantial underreporting of the extent of democratic support in 91 country-years.

While these issues involve mistakes in recording the numerator of the percentage, the number of respondents who provided a democracy-supporting answer, entering the denominator, the total number of respondents asked a question, was also problematic on occasion.
For example, when the Americas Barometer surveyed Canada in 2010, it included an item asking whether, when "democracy doesn't work," Canadians "need a strong leader who doesnâ€™t have to be elected through voting."
It only posed this question to half of its sample.
Those who were not asked the question, however, were included in the total number of respondents as if they had refused to answer.
According to the study's coding rules, refusing to answer is equivalent to answering in a fashion not supporting democracy, that is, in this case, agreeing that Canada needed a strong leader who need not bother with elections.
This might be a reasonable coding choice, but including in this category those who were never asked the question at all is clearly a mistake in data entry.

Another source of data-entry errors in this study involves survey weights.
Weighting raw survey results to maximize the extent to which they are representative of the target population is important.
Relying on the toplines reported in survey codebooks rather than the survey data itself evidently caused some mistakes in correctly entering the needed information here, as codebooks do not always take survey weights into account.
These data-entry errors shifted the percentage of democracy-supporting responses in both directions, typically by relatively small amounts.


## Consequences for Inference

Data-entry errors of this sort can yield erroneous conclusions.
A now-classic literature maintains that experience with democratic governance generates robust public support for democracy [see, e.g., @Lipset1959a].
@Claassen2020b argued instead that democratic support behaves thermostatically, that is, that increases in democracy yield an authoritarian backlash in the public, while democratic backsliding prompts the public to rally to democracy's cause.
We replicated each of the models presented in @Claassen2020b exactly, first with the article's original dataset and then making the corrections to the errors in the data we described above.
The results provide only limited support for the classic argument and none at all for a thermostatic relationship.

Figure \@ref(fig:regressionPlot) presents our results in a "small multiple" plot [@SoltHu2015] for a clear comparison of the coefficients of each variable across the models. 
In the plot, the dots represent point estimates and the whiskers show the associated 95% confidence intervals.
Each row depicts a variable's performance in its own scale across all of the models.
The lighter dots and whiskers replicate those reported in the published article; the darker ones are the estimates obtained when using the corrected data.


```{r regressionResult}
sd.plm <- pdata.frame(df_apsr, index = c("Country", "Year")) 
sd.plm2 <- pdata.frame(df_apsrCorrected, index = c("Country", "Year"))

ls_ivECM <- rep(c("diff(Libdem_z, lag = 1) + plm::lag(Libdem_z, 1)", 
                  "diff(Polyarchy_z, lag = 1) + plm::lag(Polyarchy_z, 1) + diff(Liberal_z, lag = 1) + plm::lag(Liberal_z, 1)"),
                each = 2) %>% 
  paste0(rep(c("",
               " + diff(Corrup_TI_z, lag = 1) + plm::lag(Corrup_TI_z, 1)"),
             times = 2))

ls_eqECM <- glue("diff(SupDem_trim, lag = 1) ~ plm::lag(SupDem_trim, 1:2) + {ls_ivECM} + diff(lnGDP_imp, lag = 1) + plm::lag(lnGDP_imp, 1)") %>% 
  as.vector()

ls_ivFD <- rep(c("Libdem_z",
                 "Polyarchy_z + Liberal_z"), 
               each = 2) %>% 
  paste0(rep(c("",
               " + Corrup_TI_z"),
             times = 2))

ls_eqFD <- glue("SupDem_trim ~ {ls_ivFD} + lnGDP_imp") %>% 
  as.vector()

ls_mod_original <- c(
  glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm)"),
  glue("plm({ls_eqFD}, model = 'fd', index = 'Country', data = sd.plm)")
)
ls_mod_correct <- c(
  glue("plm({ls_eqECM}, model = 'pooling', data = sd.plm2)"),
  glue("plm({ls_eqFD}, model = 'fd', index = 'Country', data = sd.plm2)")
)

ls_mod <- c(ls_mod_original, ls_mod_correct)

result_clsAPSR <- map(ls_mod, ~eval(parse(text = .)))

result_clsAPSR_tidy <- map(result_clsAPSR, function(aResult) {
  tidy_result <- tidy(aResult, conf.int = TRUE) %>% 
    mutate(std.error = vcovHC_se(aResult))
  
  glance_result <- glance.plm(aResult)
  
  ls_result <- list(tidy_result, glance_result)
})
```

```{r regressionPlot, fig.cap= "The Effect of Democracy on Change in Public Support", fig.width=7, fig.height=9.5}

txt_models <- paste0("Model ",
                     rep(1:2, each = 4), ".", rep(1:4, 2),
                     " (",
                     rep(c("EC", "EC", "FD", "FD"), 2),
                     ")")

vec_numCoef <- map_dbl(c(1, 2, 5, 6, 3, 4, 7, 8), # ECM ECM FD FD
                       ~nrow(result_clsAPSR_tidy[[.]][[1]])) - 1 # subtract the intercepts

txt_model_lab <- map2(txt_models, vec_numCoef, ~rep(.x, times = .y)) %>% 
    unlist # model lists
txt_model_lab <- c(txt_model_lab[str_which(txt_model_lab, "EC")], 
                   txt_model_lab[str_which(txt_model_lab, "FD")])
# Modify the orders to match the dataset

index_coefName <- c(
    `plm::lag(SupDem_trim, 1:2)1` = "Democratic~Support[t-1]",
    `plm::lag(SupDem_trim, 1:2)2` = "Democratic~Support[t-2]",
    `diff(Libdem_z, lag = 1)` = "Delta~Liberal~Democracy",
    `Libdem_z` = "Delta~Liberal~Democracy",
    `plm::lag(Libdem_z, 1)` = "Liberal~Democracy[t-1]",
    `diff(Polyarchy_z, lag = 1)` = "Delta~Electoral~Democracy",
    `Polyarchy_z` = "Delta~Electoral~Democracy",
    `plm::lag(Polyarchy_z, 1)` = "Electoral~Democracy[t-1]",
    `diff(Liberal_z, lag = 1)` = "Delta~Minoritarian~Democracy",
    `Liberal_z` = "Delta~Minoritarian~Democracy",
    `plm::lag(Liberal_z, 1)` = "Minoritarian~Democracy[t-1]",
    `diff(lnGDP_imp, lag = 1)` = "Delta~Log~GDP~per~capita",
    `lnGDP_imp` = "Delta~Log~GDP~per~capita",
    `plm::lag(lnGDP_imp, 1)` = "Log~GDP~per~capita[t-1]",
    `diff(Corrup_TI_z, lag = 1)` = "Delta~Corruption",
    `Corrup_TI_z` = "Delta~Corruption",
    `plm::lag(Corrup_TI_z, 1)` = "Corruption[t-1]"
)

df_plot <- map_dfr(result_clsAPSR_tidy, ~.[[1]]) %>% 
    filter(term != "(Intercept)") %>%
    mutate(
        type = ifelse(str_detect(term, "(_z|imp)$"), "FD", "EC"),
        term = index_coefName[term],
        term = factor(term, levels = unique(index_coefName)),
        submodel = rep(c("Published", "Corrected"), each = nrow(.)/2),
        submodel = factor(submodel, levels = c("Published","Corrected")),
        model = rep(txt_model_lab, times = 2),
        model = factor(model, levels = txt_models)
    )

txt_caption <- strwrap("Notes: Replications of Claassen (2020), Table 1, 47, and Table 2, 49.  Models denoted 'EC' are error-correction models; those marked 'FD' are first-difference models.", width = 115) %>% 
  paste0(sep="", collapse="\n")

small_multiple(df_plot, axis_switch = TRUE, dot_args = list(size = .9, fatten = 1)) +
    geom_hline(yintercept = 0, colour = "grey50", linetype = 2) +
    scale_color_grey(start = 0.8, end = 0.4, 
                     name = NULL,
                     breaks = c("Published",
                              "Corrected"),
                     labels = c("Publication Data",
                              "Corrected Data")) +
    theme(axis.text.x  = element_text(angle = 90, hjust = .5),
          strip.text.y.left = element_text(angle = 0),
          legend.position = c(0.006, -.01),
          legend.justification = c(1, 1), 
          legend.title = element_blank(),
          legend.title.align = 0.5,
          legend.text = element_text(size = 8),
          legend.background = element_rect(color="gray90"),
          legend.spacing = unit(-5, "pt"),
          legend.key.width = unit(4.5, "mm"),
          legend.key.height = unit(7, "mm"),
          plot.title.position = "plot",
          plot.caption.position = "plot",
          plot.caption = element_text(size = 10, hjust = 0)) +
  guides(color = guide_legend(override.aes = list(size=.3))) +
  ggtitle("Dependent Variable: Change in Public Democratic Support") +
  labs(caption = txt_caption)

ggsave("images/results.png")
```

Consider first Models 1.1 through 1.4, which replicate those presented in Table 1 of @Claassen2020b [, 47].
These models examine the effects of overall liberal democracy using error-correction models (labeled EC in Figure~\@ref(fig:regressionPlot)) and first-difference models (labeled FD).
As @Claassen2020b [, 46] notes, the thermostatic theory predicts that the estimated coefficient of the change in liberal democracy will be negative, while the classic theory suggests that lagged levels of liberal democracy will be positive.
When using the original publication data with their data-entry errors, the coefficients estimated for the change in liberal democracy are large, negative, and statistically significant across all four models, as predicted by the thermostatic theory. 
The positive and statistically significant result for the lagged level of liberal democracy found in Model 1.1---supporting the classic theory---disappears when corruption is taken into account in Model 1.2, indicating that "this effect is not particularly robust" [@Claassen2020b, 47].

When the data-entry errors are corrected, however, the results for these models suggest a different set of conclusions.
The standard errors shrink across the board---indicating that the models are better estimated in the corrected data---but so do the magnitudes of the coefficients.
The positive and statistically significant result for the lagged level of liberal democracy remains in Model 1.1.
The estimate is only slightly smaller than in the publication data, and as with the publication data, it disappears when corruption is added in Model 1.2: the evidence for the classic theory, such as it is, remains substantively unchanged.
On the other hand, the estimates for the change in liberal democracy that provided support for the thermostatic theory are much smaller---very nearly exactly zero---and fail to reach statistical significance in any of these four models.
Models 2.1 through 2.4, which break liberal democracy into its electoral democracy and minoritarian democracy components, similarly undermine claims for the thermostatic theory.
The strong and statistically significant negative coefficients for the change in minoritarian democracy on public democratic support that are found using the publication dataset evaporate when the data-entry errors are corrected.
There is no support for the thermostatic theory.


# Discussion

We draw three conclusions from the foregoing.
First, researchers should minimize reliance on manual data entry and maximize the extent to which data collection and wrangling---the 'janitor work' of data analysis---is performed computationally.
Automating 'janitor work' will sometimes require considerable programming effort, but often software is available that makes the task straightforward, such as the `readtext` R package for formatting the contents of text files for text analysis [@Benoit2021] or the `DCPOtools` R package for wrangling survey data to be used in latent variable analyses [@Solt2018].
In addition to minimizing data-entry errors, writing computer code that starts from the raw source material and works forward has the added benefit of making research much more replicable [see, e.g., @Benoit2016].
As @Christensen2019 [, 197] admonish, ``Write code instead of working by hand . . . don't use Microsoft Excel if it can be avoided."

Second, when manual data entry _cannot_ be avoided, each entry should be made twice, either by different people working independently or by the same person working at a different time, to allow for cross-checking.
Double entry is labor intensive, but experiments have shown that while visually inspecting entered data is no better at catching mistakes than simply entering the data once and making no checks, the double-entry approach reduces error rates by thirty-fold [@Barchard2011, 1837].
Given that, as shown above, data-entry errors can completely undermine the validity of our conclusions, double entry is worth the extra effort.

Finally...

# References